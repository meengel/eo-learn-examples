{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f84f86f",
   "metadata": {},
   "source": [
    "# GEM ML Framework Demonstrator - Deforestation Detection\n",
    "In these notebooks, we provide an in-depth example of how the GEM ML framework can be used for segmenting deforested areas using Sentinel-2 imagery as input and the [TMF dataset](https://forobs.jrc.ec.europa.eu/TMF/) as a reference.\n",
    "The idea is to use a neural network (NN) model for the analysis.\n",
    "Thanks to the flexibility of the GEM ML framework, we can easily substitute the model in the future by adjusting only the configuration file.\n",
    "We will have a look at the following notebooks separately:\n",
    "- 00_Configuration\n",
    "- 01_DataAcquisition\n",
    "- 02_DataNormalization\n",
    "- 03_TrainingValidationTesting\n",
    "- 04_Inference_Clouds\n",
    "\n",
    "Authors: Michael Engel (m.engel@tum.de) and Joana Reuss (joana.reuss@tum.de)\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "# Inference - Clouds\n",
    "This notebook shows how the GEM ML Framework can support continuous deforestation monitoring. In the chosen area, deforestation takes place for the sake of bauxite mining, and, e.g. a land surveying office is asking for an analysis.\n",
    "Reference data is not available, and several observations are cloudy. Still, we want a fast and reliable segmentation map of the area - preferably cloudless! For that purpose, a fast inference pipeline is necessary.\n",
    "For that purpose, we can use the `ModelForwardTask` method provided within the `PyTorchTasks` module.\n",
    "It enables the users to integrate an already trained PyTorch-model into their eo-learn workflows.\n",
    "The provided `ExecuteME` package does the management of GPU/CPU shifting.\n",
    "In general, we recommend doing this in standard Python scripts as Jupyter Notebooks do not support the spawn method for parallelization that PyTorch-objects ask for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c981409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import natsort\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorboard import notebook\n",
    "\n",
    "from sentinelhub import SHConfig, BBox, CRS, DataCollection, UtmZoneSplitter, DataCollection\n",
    "from eolearn.core import FeatureType, EOPatch, MergeEOPatchesTask, MapFeatureTask, MergeFeatureTask, ZipFeatureTask, LoadTask, EONode, EOWorkflow, EOExecutor, OverwritePermission, SaveTask\n",
    "from eolearn.io import SentinelHubDemTask, ExportToTiffTask, SentinelHubInputTask, SentinelHubEvalscriptTask, get_available_timestamps, ImportFromTiffTask\n",
    "from eolearn.mask import CloudMaskTask, JoinMasksTask\n",
    "from eolearn.features.feature_manipulation import SpatialResizeTask\n",
    "from eolearn.features.utils import ResizeMethod, ResizeLib\n",
    "\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon,Point\n",
    "import folium\n",
    "from folium import plugins as foliumplugins\n",
    "\n",
    "from libs.ConfigME import Config, importME\n",
    "from libs.MergeTDigests import mergeTDigests\n",
    "from libs.QuantileScaler_eolearn import QuantileScaler_eolearn_tdigest\n",
    "from libs.Dataset_eolearn import Dataset_eolearn\n",
    "from libs import AugmentME\n",
    "from libs import ExecuteME\n",
    "\n",
    "from tasks.TDigestTask import TDigestTask\n",
    "from tasks.PickIdxTask import PickIdxTask\n",
    "from tasks.SaveValidTask import SaveValidTask\n",
    "from tasks.PyTorchTasks import ModelForwardTask\n",
    "\n",
    "from utils.rasterio_reproject import rasterio_reproject\n",
    "from utils.transforms import batchify, predict, mover, Torchify\n",
    "from utils.parse_time_interval_observations import parse_time_interval_observations\n",
    "\n",
    "print(\"Working Directory:\",os.getcwd())\n",
    "print(\"Environment:\",os.environ['CONDA_DEFAULT_ENV'])\n",
    "print(\"Executable:\",sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484bf33",
   "metadata": {},
   "source": [
    "# Config\n",
    "First, we load our configuration file which provides all information we need throughout the script and linuxify our paths (if you are working on a Windows machine) as the eo-learn filesystem manager does not support backslashes for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load configuration file\n",
    "config = Config.LOAD(\"config.dill\")\n",
    "\n",
    "#%% linuxify\n",
    "config.linuxify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4838c8b3",
   "metadata": {},
   "source": [
    "# Area of Interest\n",
    "Let's load the geojson of our area of interest for our use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load geojson files\n",
    "aoi_showcase = gpd.read_file(config['AOI_showcase'])\n",
    "\n",
    "#%% find best suitable crs and transform to it\n",
    "crs_showcase = aoi_showcase.estimate_utm_crs()\n",
    "aoi_showcase = aoi_showcase.to_crs(crs_showcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee204fe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% calculate and print size\n",
    "aoi_showcase_shape = aoi_showcase.geometry\n",
    "aoi_showcase_width = [geom.bounds[2]-geom.bounds[0] for geom in aoi_showcase_shape]\n",
    "aoi_showcase_height = [geom.bounds[3]-geom.bounds[1] for geom in aoi_showcase_shape]\n",
    "print(f\"Dimension of the showcase area is {np.sum(np.array(aoi_showcase_width)*np.array(aoi_showcase_height)):.2e} m2\")\n",
    "\n",
    "#%% create a splitter to obtain a list of bboxes\n",
    "bbox_splitter_showcase = UtmZoneSplitter(aoi_showcase_shape, aoi_showcase.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "\n",
    "bbox_list_showcase = np.array(bbox_splitter_showcase.get_bbox_list())\n",
    "info_list_showcase = np.array(bbox_splitter_showcase.get_info_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9bbe5",
   "metadata": {},
   "source": [
    "# Visualization of AOI\n",
    "Please note that this part is not necessary for the analysis but we highly recommend doing these type of things!\n",
    "Again, we define our entry points for later parallelization on Windows machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #%% determine number of coordinate reference systems\n",
    "    crss = [bbox_._crs for bbox_ in bbox_list_showcase]\n",
    "    crss_unique = np.array(list(dict.fromkeys(crss)))\n",
    "    n_crss = len(crss_unique)\n",
    "\n",
    "    #%% sort geometries and indices by crs and store to disk\n",
    "    geometries = [[] for i in range(n_crss)]\n",
    "    idxs = [[] for i in range(n_crss)]\n",
    "    idxs_x = [[] for i in range(n_crss)]\n",
    "    idxs_y = [[] for i in range(n_crss)]\n",
    "    for i,info in enumerate(info_list_showcase):\n",
    "        idx_ = np.argmax(crss_unique==bbox_list_showcase[i]._crs)\n",
    "\n",
    "        geometries[idx_].append(Polygon(bbox_list_showcase[i].get_polygon())) # geometries sorted by crs\n",
    "        idxs[idx_].append(info[\"index\"]) # idxs sorted by crs\n",
    "        idxs_x[idx_].append(info[\"index_x\"]) # idxs_x sorted by crs\n",
    "        idxs_y[idx_].append(info[\"index_y\"]) # idxs_y sorted by crs\n",
    "\n",
    "    tiles = []\n",
    "    for i in range(n_crss):\n",
    "        #%%% build dataframe of our areas of interest (and each crs)\n",
    "        tiles.append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\"index\": idxs[i], \"index_x\": idxs_x[i], \"index_y\": idxs_y[i]},\n",
    "                crs=\"EPSG:\"+crss_unique[i]._value_,\n",
    "                geometry=geometries[i]\n",
    "            )\n",
    "        )\n",
    "        #%%% save dataframes to shapefiles\n",
    "        tiles[-1].to_file(os.path.join(config[\"dir_results\"],f\"grid_aoi_showcase_{i}_EPSG{str(crss_unique[i]._value_)}.gpkg\"), driver=\"GPKG\")\n",
    "\n",
    "    #%% print amount of patches\n",
    "    print(\"Total number of tiles:\",len(bbox_list_showcase))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caa372",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #%% visualize using folium\n",
    "    aoi_folium = aoi_showcase.to_crs(\"EPSG:4326\")\n",
    "    location = [aoi_folium.centroid.y,aoi_folium.centroid.x]\n",
    "\n",
    "    mapwindow = folium.Map(location=location, tiles='Stamen Terrain', zoom_start=8)\n",
    "\n",
    "    #%%% add aois\n",
    "    #%%%% train\n",
    "    mapwindow.add_child(\n",
    "        folium.features.Choropleth(\n",
    "            aoi_folium.to_json(),\n",
    "            fill_color=\"royalblue\",\n",
    "            nan_fill_color=\"royalblue\",\n",
    "            fill_opacity=0,\n",
    "            nan_fill_opacity=0.5,\n",
    "            line_color=\"royalblue\",\n",
    "            line_weight=1,\n",
    "            line_opacity=0.6,\n",
    "            smooth_factor=5,\n",
    "            name=f\"showcase area\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #%%% add grids in color\n",
    "    for t_,tiles_ in enumerate(tiles):\n",
    "        cp = folium.features.Choropleth(\n",
    "                tiles_.to_crs(\"EPSG:4326\").to_json(),\n",
    "                fill_color=\"royalblue\",\n",
    "                nan_fill_color=\"black\",\n",
    "                fill_opacity=0,\n",
    "                nan_fill_opacity=0.5,\n",
    "                line_color=\"royalblue\",\n",
    "                line_weight=0.5,\n",
    "                line_opacity=0.6,\n",
    "                smooth_factor=5,\n",
    "                name=f\"showcase grid EPSG:{crss_unique[t_]._value_}\"\n",
    "            ).add_to(mapwindow)\n",
    "\n",
    "        # display index next to cursor\n",
    "        folium.GeoJsonTooltip(\n",
    "            ['index'],\n",
    "            aliases=['Index:'],\n",
    "            labels=False,\n",
    "            style=\"background-color:rgba(0,101,189,0.4); border:2px solid white; color:white;\",\n",
    "            ).add_to(cp.geojson)\n",
    "\n",
    "    #%%% add some controls\n",
    "    folium.LayerControl().add_to(mapwindow)\n",
    "    foliumplugins.Fullscreen(force_separate_button=True).add_to(mapwindow)\n",
    "\n",
    "    #%%% save, render and display\n",
    "    mapwindow.save(os.path.join(config[\"dir_results\"],'gridmap_showcase.html'))\n",
    "    mapwindow.render()\n",
    "    mapwindow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd25ec",
   "metadata": {},
   "source": [
    "# eo-learn worlflow\n",
    "Similar to the workflow we have defined in [01_DataAcquisition]().\n",
    "\n",
    "It again consists of the following elements\n",
    "\n",
    "- __EOTask__\n",
    "- __EONode__\n",
    "- __EOWorkflow__\n",
    "\n",
    "This time, we define the following `EOTasks`:\n",
    "\n",
    "\n",
    "##  1. Input data: Querying Sentinel data\n",
    ">- __*task_data*__: We take a [Sentinel-Hub-Input-Task](https://eo-learn.readthedocs.io/en/latest/eolearn.io.sentinelhub_process.html#eolearn.io.sentinelhub_process.SentinelHubInputTask) for querying **Sentinel-2 data**.\n",
    "\n",
    "## 2. PyTorch tasks\n",
    ">- __*task_model*__: We take a [ModelForwardTask]() for defining the model's forward function.\n",
    "\n",
    "## 3. Cloud removal\n",
    ">- __*task_postprocessing*__: We remove clouds from the prediction using a [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask).\n",
    "\n",
    "\n",
    "## 4. Exporting and Saving\n",
    "#### 4.1 Exporting\n",
    ">- __*task_tiff*__: We export the model's predictions as tif-files using an [`ExportToTiffTask`](https://eo-learn.readthedocs.io/en/latest/reference/eolearn.io.raster_io.html#eolearn.io.raster_io.ExportToTiffTask).\n",
    ">- __*task_tiff_postprocessing*__: We export the model's predictions without clouds as tif-files using an [`ExportToTiffTask`](https://eo-learn.readthedocs.io/en/latest/reference/eolearn.io.raster_io.html#eolearn.io.raster_io.ExportToTiffTask).\n",
    "\n",
    "#### 4.2 Saving\n",
    ">- __*task_save*__: We save the created EOPatches using a __`SaveValidTask`__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8e53e",
   "metadata": {},
   "source": [
    "## 1. Input data: Querying Sentinel data\n",
    "First, we define our `EOTasks` for the input data. They are the same as for the training, validation and testing procedure. Except that we skip the checking and reference this time (as there is none).\n",
    "\n",
    "Please note that we do not apply a cloud filtering here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cdb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sentinel-Hub-Input-Task\n",
    "task_data = SentinelHubInputTask(\n",
    "    data_collection = DataCollection.SENTINEL2_L1C,\n",
    "    size = None,\n",
    "    resolution = config[\"resolution\"],\n",
    "    bands_feature = (FeatureType.DATA, \"data\"),\n",
    "    bands = [\"B02\",\"B03\",\"B04\",\"B08\",\"B11\",\"B12\"],\n",
    "    additional_data = [(FeatureType.MASK, \"dataMask\", \"dmask_data\"),(FeatureType.MASK, \"CLM\", \"cmask_data\")],\n",
    "    evalscript = None,\n",
    "    maxcc = 1,\n",
    "    time_difference = dt.timedelta(hours=1),\n",
    "    cache_folder = config[\"dir_cache\"],\n",
    "    max_threads = config[\"threads\"],\n",
    "    config = config[\"SHconfig\"],\n",
    "    bands_dtype = np.float32,\n",
    "    single_scene = False,\n",
    "    mosaicking_order = \"mostRecent\",\n",
    "    aux_request_args = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff44ea",
   "metadata": {},
   "source": [
    "## 2. PyTorch Tasks\n",
    "As a first step, however, we need to load our Scaler as built in [02_DataNormalization]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler = QuantileScaler_eolearn_tdigest.LOAD(os.path.join(config[\"dir_results\"],config[\"savename_scaler\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e972e",
   "metadata": {},
   "source": [
    "Subsequently, we load our __best model__ using the `BaseClass` of `AugmentME`.\n",
    "Further, we set it to evaluation mode and tell it to share its memory for being deployed on multiple CPUs.\n",
    "Loading the model to the `CPU` is essential as you get in trouble with the parallelization of `ExecuteME` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AugmentME.BaseClass(mode=\"torch\")\n",
    "model.load(os.path.join(config[\"dir_results\"],config[\"model_savename_bestloss\"]),device=\"cpu\")\n",
    "model.eval()\n",
    "model.share_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5978c8",
   "metadata": {},
   "source": [
    "### ModelForwardTask\n",
    "As discussed, we want to respond to the request immediately.\n",
    "That means, we want to use our trained, validated and tested model for prediction!\n",
    "Fortunately, TUM established the `PyTorchTask` as a base class for many PyTorch related `EOTasks` like the `ModelForwardTask`, the `LayerGradCamTask` or the `GradientShapTask`, for example.\n",
    "In this notebook, we focus on the `ModelForwardTask` since we need to implement a functionality for returning the model's prediction, as the model's regular forward method only returns the logits and not the final prediction.\n",
    "This ensures that the input feature is fed to our model and the output is returned as intended.\n",
    "\n",
    "As for all `EOTasks`, we choose the in- and output features.\n",
    "The `in_feature` is fed to the model, whereas its result is stored in the `out_feature`.\n",
    "In order to properly normalize the downloaded data, we have to insert our scaler for `in_transform`.\n",
    "Since we are interested in the predicted land cover, we have to insert our prediction transform for `out_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_model = ModelForwardTask(\n",
    "    in_feature = (FeatureType.DATA,\"data\"),\n",
    "    out_feature = (FeatureType.MASK,\"model_output\"),\n",
    "    model = model,\n",
    "\n",
    "    in_transform = Scaler,\n",
    "    out_transform = predict,\n",
    "    in_torchtype = torch.FloatTensor,\n",
    "    batch_size = config[\"max_batch_size\"],\n",
    "\n",
    "    maxtries=3,\n",
    "    timeout=22,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280e0da",
   "metadata": {},
   "source": [
    "## 3. Cloud Removal\n",
    "Since our scenario represents a request made by a land surveying office, we preferably do not want to have clouds in our analysis while sticking to the robustness of our model using multispectral satellite data.\n",
    "Fortunately, we defined our labels accordingly: by using the maximum predicted value out of a series of predictions, we always remove a cloud (class 0) if a less cloudy observation is available, meaning a class value greater 0.\n",
    "Further, we always overwrite forest (class 2) by deforestation (class 3), since it is unlikely that a full forest follows deforestation.\n",
    "By doing so, we always catch the deforestation which took place in a certain time period which is chosen by the user within the workflow arguments.\n",
    "That post-processing is done using the [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa11626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer(reference):\n",
    "    return np.max(reference,axis=0)\n",
    "task_postprocessing = MapFeatureTask(\n",
    "    input_features = (FeatureType.MASK, \"model_output\"),\n",
    "    output_features = (FeatureType.MASK_TIMELESS, \"model_output_post\"),\n",
    "    map_function = maximizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160f27f",
   "metadata": {},
   "source": [
    "## 4. Exporting and saving\n",
    "#### 4.1 Exporting\n",
    "Of course, we want to export the both the model's output as a GeoTiff for others to analyze it using a common GIS software.\n",
    "\n",
    "We can use the [`ExportToTiffTask`](https://eo-learn.readthedocs.io/en/latest/reference/eolearn.io.raster_io.html#eolearn.io.raster_io.ExportToTiffTask) in order to export both the model's output before and after removing clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% export raw model output\n",
    "task_tiff = ExportToTiffTask(\n",
    "    feature = (FeatureType.MASK,\"model_output\"),\n",
    "    folder = config[\"dir_tiffs_showcase\"],\n",
    "    date_indices = None,\n",
    "    band_indices = None,\n",
    "    crs = None,\n",
    "    fail_on_missing = True,\n",
    "    compress = \"deflate\"\n",
    ")\n",
    "\n",
    "#%% export post-processed model output\n",
    "task_tiff_postprocessing = ExportToTiffTask(\n",
    "    feature = (FeatureType.MASK_TIMELESS,\"model_output_post\"),\n",
    "    folder = config[\"dir_tiffs_showcase\"],\n",
    "    date_indices = None,\n",
    "    band_indices = None,\n",
    "    crs = None,\n",
    "    fail_on_missing = True,\n",
    "    compress = \"deflate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a68cf7",
   "metadata": {},
   "source": [
    "### 4.2 Saving\n",
    "Finally, we want to store the resulting patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb358599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save EOPatches\n",
    "task_save = SaveTask(\n",
    "    path = config[\"dir_data\"],\n",
    "    filesystem = None,\n",
    "    config = config[\"SHconfig\"],\n",
    "    overwrite_permission = OverwritePermission.OVERWRITE_PATCH,\n",
    "    compress_level = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fc9b2",
   "metadata": {},
   "source": [
    "## EONodes\n",
    "After we have defined all necessary __`EOTasks`__, we initialize the __`EONodes`__ which will be used in order to run through the workflow afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% input nodes\n",
    "node_data = EONode(\n",
    "    task = task_data,\n",
    "    inputs = [],\n",
    "    name = \"load Sentinel-2 data\"\n",
    ")\n",
    "\n",
    "#%% inference node\n",
    "node_model = EONode(\n",
    "    task = task_model,\n",
    "    inputs = [node_data],\n",
    "    name = \"predict water mask\"\n",
    ")\n",
    "\n",
    "node_postprocessing = EONode(\n",
    "    task = task_postprocessing,\n",
    "    inputs = [node_model],\n",
    "    name = \"predict water mask\"\n",
    ")\n",
    "\n",
    "#%% export and save\n",
    "node_tiff = EONode(\n",
    "    task = task_tiff,\n",
    "    inputs = [node_postprocessing],\n",
    "    name = \"export GeoTiff of model output\"\n",
    ")\n",
    "\n",
    "node_tiff_postprocessing = EONode(\n",
    "    task = task_tiff_postprocessing,\n",
    "    inputs = [node_tiff],\n",
    "    name = \"export GeoTiff of model output\"\n",
    ")\n",
    "\n",
    "node_save = EONode(\n",
    "    task = task_save,\n",
    "    inputs = [node_tiff_postprocessing],\n",
    "    name = \"save EOPatch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0819005",
   "metadata": {},
   "source": [
    "## Final EOWorkflow\n",
    "\n",
    "Finally, we can define our workflow using the end node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56870be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = EOWorkflow.from_endnodes(node_save)\n",
    "#workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa40b6",
   "metadata": {},
   "source": [
    "## Execution\n",
    "So far, we have defined our\n",
    "- Area of Interest\n",
    "- Input tasks\n",
    "- Model tasks\n",
    "- EOWorkflow\n",
    "\n",
    "What is left, is the definition of our __execution (or workflow) arguments__.\n",
    "We want to execute our workflow in parallel.\n",
    "This can be done using the package `ExecuteME`.\n",
    "\n",
    "Since PyTorch models demand the spawn start method for subprocesses, we must ensure the entry point.\n",
    "This is accomplished by setting the file that defines the subprocesses as the main file.\n",
    "We recommend doing this in standard Python scripts, as Jupyter Notebooks require clarifying the entry point in every cell (so you can easily export it as a script) and do not support the spawn method.\n",
    "That is, the parallelization does not work in Jupyter Notebooks, but you may let it run using one worker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8a73d",
   "metadata": {},
   "source": [
    "### Workflow Arguments\n",
    "First, we have to define __workflow arguments__, both temporal and spatial.\n",
    "Note that we only want to download the data that is not yet present on our device.\n",
    "Hence, we check for existence first and assign arguments afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b684889",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':  \n",
    "    #%% define workflow arguments\n",
    "    workflow_args = []\n",
    "    bbox_list_ = bbox_list_showcase\n",
    "    for i in range(len(bbox_list_)):\n",
    "        print(f\"\\rChecking workflow args {i+1}/{len(bbox_list_)}\",end=\"\\r\")\n",
    "        try:\n",
    "            timeinterval = (config[\"start_showcase\"],config[\"end_showcase\"])\n",
    "            timeintervalstring = f\"{timeinterval[0].strftime(r'%Y-%m-%dT%H-%M-%S_%Z')}--{timeinterval[1].strftime(r'%Y-%m-%dT%H-%M-%S_%Z')}\"\n",
    "            dir_ = f\"showcase/eopatch_{i}_{timeintervalstring}\"\n",
    "            if not os.path.exists(os.path.join(config[\"dir_data\"],dir_)):### and False: ### \n",
    "                workflow_args.append(\n",
    "                    {\n",
    "                        node_data: {\"bbox\":bbox_list_[i],\"time_interval\":timeinterval},\n",
    "                        node_tiff: {\"filename\": f\"deforestation_raw_{i}_{timeintervalstring}\"},\n",
    "                        node_tiff_postprocessing: {\"filename\": f\"deforestation_postprocessed_{i}_{timeintervalstring}\"},\n",
    "                        node_save: {\"eopatch_folder\":dir_}\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print()\n",
    "\n",
    "    print(f\"Number of downloads/calculations: {len(workflow_args)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05bd64f",
   "metadata": {},
   "source": [
    "### Devices\n",
    "Secondly, we need to initialize a multiprocessor queue containing our devices' names.\n",
    "This way, we can use any number of GPUs.\n",
    "Of course, it can also stay on the CPU by setting the config.device accordingly.\n",
    "\n",
    "The `PyTorchTasks` can be used for features that contain multiple timestamps to be analyzed.\n",
    "Accordingly, the `batch_size` parameter of the `ModelForwardTask` refers to the timestamps, i.e., the first dimension of a feature array.\n",
    "If you do not have multiple timestamps, you can insert a kind of `batch_size` for a device by defining `batch_size x available_devices` devices or especially `batch_size` times the device you want to use multiple times.\n",
    "Please be careful with this as there is an additional cost per patch to initialize the model since the model is not shared between multiple `EOPatches`.\n",
    "Nevertheless, this behavior is advantageous if an analysis is to be carried out using the `ModelUncertaintyTask`, for example.\n",
    "Here it is necessary to use the model several times anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':  \n",
    "    devices = ExecuteME.Devices([\"cuda\"],multiprocessing_context=\"spawn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556f405",
   "metadata": {},
   "source": [
    "In the next step, we will define the multiprocessor keyword arguments that must be passed to our tasks separately. These keyword arguments must be shared between the processes because the list of available devices should be known to the different processes and, hence, be shared or provided separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e604f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':  \n",
    "    mpkwargs = {\n",
    "        node_model: {\"devices\":devices},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b0d3b",
   "metadata": {},
   "source": [
    "### Run\n",
    "Now, it's time to let it run!\n",
    "Please notice that we insert a 0 for `threads` since Jupyter Notebooks do not allow for the spawn method.\n",
    "In a Python script, you may choose as many threads as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8c4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':  \n",
    "    start_multi = time.time()\n",
    "    results_multi = ExecuteME.execute(\n",
    "        fun = workflow.execute,\n",
    "        kwargslist = workflow_args,\n",
    "        mpkwargs = mpkwargs,\n",
    "        \n",
    "        kwargsmode = None,\n",
    "        resultsqueue = None,\n",
    "        NoReturn = True,\n",
    "        \n",
    "        timeout = 1,\n",
    "        threads = 0, #config[\"threads\"],\n",
    "        checkthreads = True,\n",
    "        multiprocessing_context = \"spawn\",\n",
    "        multiprocessing_mode = \"std\",\n",
    "        bequiet = False\n",
    "    )\n",
    "    time_multi = time.time()-start_multi\n",
    "\n",
    "    #%%% results\n",
    "    print(f\"Time Multi:\\t\\t{time_multi}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a05e9",
   "metadata": {},
   "source": [
    "### Downloaded Data\n",
    "Let's have a look at how many `EOPatches` got stored to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':  \n",
    "    print(f\"Number of showcasedownloads: {len(workflow_args)}\")\n",
    "    print(f\"Number of stored showcase EOPatches: {len(os.listdir(config['dir_showcase']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b65752",
   "metadata": {},
   "source": [
    "We finally made it!\n",
    "Everything is ready for being analyzed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c511794",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "As a first analysis step, we want to merge all of our computed GeoTiffs, both for the raw and postprocessed model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':  \n",
    "    #%% merge raw model output\n",
    "    importME(\"../utils/RasterME_merge.raster_merge\")(\n",
    "        inputfiles = [\n",
    "            os.path.join(config[\"dir_tiffs_showcase\"],dir_)\n",
    "            for dir_ in os.listdir(config[\"dir_tiffs_showcase\"])\n",
    "            if \"deforestation\" in dir_.split(\"_\") and \"raw\" in dir_.split(\"_\")\n",
    "        ],\n",
    "        outputfile = os.path.join(config[\"dir_results\"],config[\"savename_showcase_tiff\"]),\n",
    "        format_option = 'COMPRESS=Deflate',\n",
    "        sparse = True,\n",
    "        #nmax_files = 10\n",
    "    )\n",
    "\n",
    "    #%% merge postprocessed model output\n",
    "    importME(\"../utils/RasterME_merge.raster_merge\")(\n",
    "        inputfiles = [\n",
    "            os.path.join(config[\"dir_tiffs_showcase\"],dir_)\n",
    "            for dir_ in os.listdir(config[\"dir_tiffs_showcase\"])\n",
    "            if \"deforestation\" in dir_.split(\"_\") and \"postprocessed\" in dir_.split(\"_\")\n",
    "        ],\n",
    "        outputfile = os.path.join(config[\"dir_results\"],config[\"savename_showcase_tiff_post\"]),\n",
    "        format_option = 'COMPRESS=Deflate',\n",
    "        sparse = True,\n",
    "        #nmax_files = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d51318",
   "metadata": {},
   "source": [
    "Now you know how to use the `ModelForwardTask` and post-processing for analysis! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
