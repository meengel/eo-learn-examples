{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc03e20",
   "metadata": {},
   "source": [
    "# GEM ML Framework Demonstrator - Deforestation Detection\n",
    "In these notebooks, we will get a feeling of how the GEM ML framework can be used for the segmentation of deforested areas using Sentinel-2 imagery as input and the [TMF dataset](https://forobs.jrc.ec.europa.eu/TMF/) as a reference.\n",
    "The idea is to use a neural network (NN) model for the analysis.\n",
    "Thanks to the flexibility of the GEM ML framework, the model used can be replaced by changing the configuration only.\n",
    "We will have a look at the following notebooks separately:\n",
    "- 00_Configuration\n",
    "- 01_DataAcquisition\n",
    "- 02_DataNormalization\n",
    "- 03_TrainingValidationTesting\n",
    "- 04_Inference_Clouds\n",
    "- 04_Inference_Timeseries\n",
    "\n",
    "by Michael Engel (m.engel@tum.de) and Joana Reuss (joana.reuss@tum.de)\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "# Data Acquisition\n",
    "Here, we define our `EOWorkflow` for the download of our desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394ea28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time\n",
    "import natsort\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorboard import notebook\n",
    "\n",
    "from sentinelhub import SHConfig, BBox, CRS, DataCollection, UtmZoneSplitter, DataCollection\n",
    "from eolearn.core import FeatureType, EOPatch, MergeEOPatchesTask, MapFeatureTask, MergeFeatureTask, ZipFeatureTask, LoadTask, EONode, EOWorkflow, EOExecutor, OverwritePermission, SaveTask\n",
    "from eolearn.io import SentinelHubDemTask, ExportToTiffTask, SentinelHubInputTask, SentinelHubEvalscriptTask, get_available_timestamps, ImportFromTiffTask\n",
    "from eolearn.mask import CloudMaskTask, JoinMasksTask\n",
    "from eolearn.features.feature_manipulation import SpatialResizeTask\n",
    "from eolearn.features.utils import ResizeMethod, ResizeLib\n",
    "\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon,Point\n",
    "import folium\n",
    "from folium import plugins as foliumplugins\n",
    "\n",
    "from libs.ConfigME import Config, importME\n",
    "from libs.MergeTDigests import mergeTDigests\n",
    "from libs.QuantileScaler_eolearn import QuantileScaler_eolearn_tdigest\n",
    "from libs.Dataset_eolearn import Dataset_eolearn\n",
    "from libs import AugmentME\n",
    "from libs import ExecuteME\n",
    "\n",
    "from tasks.TDigestTask import TDigestTask\n",
    "from tasks.PickIdxTask import PickIdxTask\n",
    "from tasks.SaveValidTask import SaveValidTask\n",
    "from tasks.PyTorchTasks import ModelForwardTask\n",
    "\n",
    "from utils.rasterio_reproject import rasterio_reproject\n",
    "from utils.transforms import batchify, predict, mover, Torchify\n",
    "from utils.parse_time_interval_observations import parse_time_interval_observations\n",
    "\n",
    "print(\"Working Directory:\",os.getcwd())\n",
    "print(\"Environment:\",os.environ['CONDA_DEFAULT_ENV'])\n",
    "print(\"Executable:\",sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa751f3e",
   "metadata": {},
   "source": [
    "# Config\n",
    "First, we load our configuration file which provides all information we need throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load configuration file\n",
    "config = Config.LOAD(\"config.dill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52610a57",
   "metadata": {},
   "source": [
    "# Area of Interest\n",
    "Let's load the geojson of our area of interests for training, validation and testing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load geojson files\n",
    "aoi_train = gpd.read_file(config['AOI_train'])\n",
    "aoi_validation = gpd.read_file(config['AOI_validation'])\n",
    "aoi_test = gpd.read_file(config['AOI_test'])\n",
    "\n",
    "#%% find best suitable crs and transform to it\n",
    "crs_train = aoi_train.estimate_utm_crs()\n",
    "aoi_train = aoi_train.to_crs(crs_train)\n",
    "aoi_train = aoi_train.buffer(config['AOIbuffer'])\n",
    "\n",
    "crs_validation = aoi_validation.estimate_utm_crs()\n",
    "aoi_validation = aoi_validation.to_crs(crs_validation)\n",
    "aoi_validation = aoi_validation.buffer(config['AOIbuffer'])\n",
    "\n",
    "crs_test = aoi_test.estimate_utm_crs()\n",
    "aoi_test = aoi_test.to_crs(crs_test)\n",
    "aoi_test = aoi_test.buffer(config['AOIbuffer'])\n",
    "\n",
    "#%% dict for query\n",
    "aois = {\"train\":aoi_train,\n",
    "        \"validation\":aoi_validation,\n",
    "        \"test\":aoi_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681b347",
   "metadata": {},
   "source": [
    "Since our **area of interests are too large**, we **split** them into a set of smaller bboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% calculate and print size\n",
    "aoi_train_shape = aoi_train.geometry\n",
    "aoi_train_width = [geom.bounds[2]-geom.bounds[0] for geom in aoi_train_shape]\n",
    "aoi_train_height = [geom.bounds[3]-geom.bounds[1] for geom in aoi_train_shape]\n",
    "print(f\"Dimension of the training area is {np.sum(np.array(aoi_train_width)*np.array(aoi_train_height)):.2e} m2\")\n",
    "aoi_validation_shape = aoi_validation.geometry\n",
    "aoi_validation_width = [geom.bounds[2]-geom.bounds[0] for geom in aoi_validation_shape]\n",
    "aoi_validation_height = [geom.bounds[3]-geom.bounds[1] for geom in aoi_validation_shape]\n",
    "print(f\"Dimension of the validation area is {np.sum(np.array(aoi_validation_width)*np.array(aoi_validation_height)):.2e} m2\")\n",
    "aoi_test_shape = aoi_test.geometry\n",
    "aoi_test_width = [geom.bounds[2]-geom.bounds[0] for geom in aoi_test_shape]\n",
    "aoi_test_height = [geom.bounds[3]-geom.bounds[1] for geom in aoi_test_shape]\n",
    "print(f\"Dimension of the test area is {np.sum(np.array(aoi_test_width)*np.array(aoi_test_height)):.2e} m2\")\n",
    "\n",
    "#%% create a splitter to obtain a list of bboxes\n",
    "bbox_splitter_train = UtmZoneSplitter(aoi_train_shape, aoi_train.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "bbox_splitter_validation = UtmZoneSplitter(aoi_validation_shape, aoi_validation.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "bbox_splitter_test = UtmZoneSplitter(aoi_test_shape, aoi_test.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "\n",
    "bbox_list_train = np.array(bbox_splitter_train.get_bbox_list())\n",
    "info_list_train = np.array(bbox_splitter_train.get_info_list())\n",
    "bbox_list_validation = np.array(bbox_splitter_validation.get_bbox_list())\n",
    "info_list_validation = np.array(bbox_splitter_validation.get_info_list())\n",
    "bbox_list_test = np.array(bbox_splitter_test.get_bbox_list())\n",
    "info_list_test = np.array(bbox_splitter_test.get_info_list())\n",
    "\n",
    "#%% dict for query\n",
    "bbox_lists = {\"train\":bbox_list_train,\n",
    "              \"validation\":bbox_list_validation,\n",
    "              \"test\":bbox_list_test}\n",
    "info_lists = {\"train\":info_list_train,\n",
    "              \"validation\":info_list_validation,\n",
    "              \"test\":info_list_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408ac31",
   "metadata": {},
   "source": [
    "The **bbox list would be sufficient** for starting the training procedure using eo-learn.\n",
    "To check if we muddled up something, however, we want to visualize it!\n",
    "Since our area of interest is rather large, we face the problem of multiple coordinate refernce systems.\n",
    "Unfortunately, **geopandas does not support multiple crs in one dataframe** as described [here](https://github.com/sentinel-hub/sentinelhub-py/issues/123).\n",
    "Hence, we have to define a set of tiles for each separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = []\n",
    "crss_uniques = []\n",
    "for _ in [\"train\",\"validation\",\"test\"]:\n",
    "    tiles.append([])\n",
    "    #%% determine number of coordinate reference systems\n",
    "    crss = [bbox_._crs for bbox_ in bbox_lists[_]]\n",
    "    crss_unique = np.array(list(dict.fromkeys(crss)))\n",
    "    crss_uniques.append(crss_unique)\n",
    "    n_crss = len(crss_unique)\n",
    "\n",
    "    #%% sort geometries and indices by crs and store to disk\n",
    "    geometries = [[] for i in range(n_crss)]\n",
    "    idxs = [[] for i in range(n_crss)]\n",
    "    idxs_x = [[] for i in range(n_crss)]\n",
    "    idxs_y = [[] for i in range(n_crss)]\n",
    "    for i,info in enumerate(info_lists[_]):\n",
    "        idx_ = np.argmax(crss_unique==bbox_lists[_][i]._crs)\n",
    "\n",
    "        geometries[idx_].append(Polygon(bbox_lists[_][i].get_polygon())) # geometries sorted by crs\n",
    "        idxs[idx_].append(info[\"index\"]) # idxs sorted by crs\n",
    "        idxs_x[idx_].append(info[\"index_x\"]) # idxs_x sorted by crs\n",
    "        idxs_y[idx_].append(info[\"index_y\"]) # idxs_y sorted by crs\n",
    "\n",
    "    for i in range(n_crss):\n",
    "        #%% build dataframe of our areas of interest (and each crs)\n",
    "        tiles[-1].append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\"index\": idxs[i], \"index_x\": idxs_x[i], \"index_y\": idxs_y[i]},\n",
    "                crs=\"EPSG:\"+crss_unique[i]._value_,\n",
    "                geometry=geometries[i]\n",
    "            )\n",
    "        )\n",
    "        #%%% save dataframes to shapefiles\n",
    "        tiles[-1][-1].to_file(os.path.join(config[\"dir_results\"],f\"grid_aoi_{_}_{i}_EPSG{str(crss_unique[i]._value_)}.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfb20c",
   "metadata": {},
   "source": [
    "We have sorted the tiles according to their corresponding crs.\n",
    "Now we want to visualize it in a nice map.\n",
    "Here, it is important to **reproject the tiles** to the crs of our **mapping application** - we do that only for this purpose, the **bbox list is not affected** by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print amount of patches\n",
    "print(\"Total number of tiles:\",[len(bbox_list) for bbox_list in bbox_lists.values()])\n",
    "\n",
    "#%% visualize using folium\n",
    "aoi_folium = aoi_validation.to_crs(\"EPSG:4326\") # use validation for visualisation\n",
    "location = [np.mean(aoi_folium.centroid.y),np.mean(aoi_folium.centroid.x)]\n",
    "\n",
    "mapwindow = folium.Map(location=location, tiles='Stamen Terrain', zoom_start=6)\n",
    "\n",
    "colors = [\"blue\",\"green\",\"red\"]\n",
    "for i,_ in enumerate([\"train\",\"validation\",\"test\"]):\n",
    "    #%%% add aois\n",
    "    #%%%% train\n",
    "    mapwindow.add_child(\n",
    "        folium.features.Choropleth(\n",
    "            aois[_].to_crs(\"EPSG:4326\").to_json(),\n",
    "            fill_color=colors[i],\n",
    "            nan_fill_color=colors[i],\n",
    "            fill_opacity=0,\n",
    "            nan_fill_opacity=0.5,\n",
    "            line_color=colors[i],\n",
    "            line_weight=1,\n",
    "            line_opacity=0.6,\n",
    "            smooth_factor=5,\n",
    "            name=f\"{_} area\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #%%% add grids in blue color\n",
    "    for t_,tiles_ in enumerate(tiles[i]):\n",
    "        cp = folium.features.Choropleth(\n",
    "                tiles_.to_crs(\"EPSG:4326\").to_json(),\n",
    "                fill_color=colors[i],\n",
    "                nan_fill_color=\"black\",\n",
    "                fill_opacity=0,\n",
    "                nan_fill_opacity=0.5,\n",
    "                line_color=colors[i],\n",
    "                line_weight=0.5,\n",
    "                line_opacity=0.6,\n",
    "                smooth_factor=5,\n",
    "                name=f\"{_} grid EPSG:{crss_uniques[i][t_]._value_}\"\n",
    "            ).add_to(mapwindow)\n",
    "\n",
    "        # display index next to cursor\n",
    "        folium.GeoJsonTooltip(\n",
    "            ['index'],\n",
    "            aliases=['Index:'],\n",
    "            labels=False,\n",
    "            style=\"background-color:rgba(0,101,189,0.4); border:2px solid white; color:white;\",\n",
    "            ).add_to(cp.geojson)\n",
    "\n",
    "#%%% add some controls\n",
    "folium.LayerControl().add_to(mapwindow)\n",
    "foliumplugins.Fullscreen(force_separate_button=True).add_to(mapwindow)\n",
    "\n",
    "#%%% save, render and display\n",
    "mapwindow.save(os.path.join(config[\"dir_results\"],'gridmap.html'))\n",
    "mapwindow.render()\n",
    "mapwindow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2158ac",
   "metadata": {},
   "source": [
    "# Input Tasks\n",
    "Now, it is time to define some input tasks for our `eo-learn` workflows.\n",
    "As an input, we will take a [Sentinel-Hub-Input-Task](https://eo-learn.readthedocs.io/en/latest/eolearn.io.sentinelhub_process.html#eolearn.io.sentinelhub_process.SentinelHubInputTask) for querying **Sentinel-2 data**.\n",
    "As a reference, we will use a [Import-From-Tiff-Task](https://eo-learn.readthedocs.io/en/latest/reference/eolearn.io.raster_io.html#eolearn.io.raster_io.ImportFromTiffTask) to load the [TMF dataset](https://forobs.jrc.ec.europa.eu/TMF/).\n",
    "For **cloud masking, or relabeling our reference,** we use the mask calculated by S2Cloudless.\n",
    "Further, we apply a labelmapping using the [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask).\n",
    "\n",
    "The date of our reference is considered to be the 2021-12-31.\n",
    "That is, we choose the observation with `config[\"maxcc\"]` cloud coverage closest to that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sentinel-Hub-Input-Task\n",
    "task_data = SentinelHubInputTask(\n",
    "    data_collection = DataCollection.SENTINEL2_L1C,\n",
    "    size = None,\n",
    "    resolution = config[\"resolution\"],\n",
    "    bands_feature = (FeatureType.DATA, \"data\"),\n",
    "    bands = [\"B02\",\"B03\",\"B04\",\"B08\",\"B11\",\"B12\"],\n",
    "    additional_data = [(FeatureType.MASK, \"dataMask\", \"dmask_data\"),(FeatureType.MASK, \"CLM\", \"cmask_data\")],\n",
    "    evalscript = None,\n",
    "    maxcc = config[\"maxcc\"],\n",
    "    time_difference = dt.timedelta(hours=1),\n",
    "    cache_folder = config[\"dir_cache\"],\n",
    "    max_threads = config[\"threads\"],\n",
    "    config = config[\"SHconfig\"],\n",
    "    bands_dtype = np.float32,\n",
    "    single_scene = False,\n",
    "    mosaicking_order = \"mostRecent\",\n",
    "    aux_request_args = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f2d92",
   "metadata": {},
   "source": [
    "In order to get the closest observation with respect to our observation date, we pick the last one only.\n",
    "Actually, we do not need that since we will query for the correct timeinterval before downloading but for the sake of safety, we use the `PickIdxTask` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Pick-Idx-Task\n",
    "task_data_pick = PickIdxTask(\n",
    "    in_feature = (FeatureType.DATA, \"data\"),\n",
    "    out_feature = None, # None for replacing in_feature\n",
    "    idx = [[-1],...] # -1 in brackets for keeping dimensions of numpy array\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e4c28",
   "metadata": {},
   "source": [
    "The same holds true for both our data- and the cloud-mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e54047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Pick-Idx-Task data mask\n",
    "task_data_pick_dmask = PickIdxTask(\n",
    "    in_feature = (FeatureType.MASK, \"dmask_data\"),\n",
    "    out_feature = None, # None for replacing in_feature\n",
    "    idx = [[-1],...] # -1 in brackets for keeping dimensions of numpy array\n",
    ")\n",
    "\n",
    "#%% Pick-Idx-Task cloud mask\n",
    "task_data_pick_cmask = PickIdxTask(\n",
    "    in_feature = (FeatureType.MASK, \"cmask_data\"),\n",
    "    out_feature = None, # None for replacing in_feature\n",
    "    idx = [[-1],...] # -1 in brackets for keeping dimensions of numpy array\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a628f1f",
   "metadata": {},
   "source": [
    "For the normalization of our dataset we will use the T-Digest algorithm.\n",
    "It is designed for quantile approximation close to the tails which we need for the common linear quantile scaler in the realm of ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% T-Digest-Task\n",
    "task_data_tdigest = TDigestTask(\n",
    "    in_feature = (FeatureType.DATA, 'data'),\n",
    "    out_feature = (FeatureType.SCALAR_TIMELESS, 'tdigest_data'),\n",
    "    mode = None,\n",
    "    pixelwise = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96db3b",
   "metadata": {},
   "source": [
    "# Reference Task\n",
    "The reference is calculated using some thresholded value of the NDWI.\n",
    "To enable the user to use other thresholds after downloading the patches, the raw NDWI and the corresponding bands will be stored within the `EOPatch` as well.\n",
    "Additionally, we will download the RGB bands for visualisation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import-From-Tiff-Task\n",
    "task_reference = ImportFromTiffTask(\n",
    "    feature = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    folder = config[\"path_reference\"],\n",
    "    use_vsi = False,\n",
    "    timestamp_size = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fb10f",
   "metadata": {},
   "source": [
    "We apply our labelmapping using the [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% apply labelmapping\n",
    "def labelmapper(reference,mapping):\n",
    "    refcopy = reference.copy()\n",
    "    for key,value in mapping.items():\n",
    "        refcopy[reference==key] = value\n",
    "    return refcopy\n",
    "task_reference_labelmapping = MapFeatureTask(\n",
    "    input_features = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    output_features = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    map_function = labelmapper,\n",
    "    mapping = config[\"labelmapping\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec284d",
   "metadata": {},
   "source": [
    "Since the reference data has been acquired using the Landsat missions with 30m resolution, we have to resize our reference to our chosen resolution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_reference_resize = SpatialResizeTask(\n",
    "    features = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    resize_parameters = ['new_size', [config[\"patchpixelwidth\"]]*2],\n",
    "    resize_method = ResizeMethod.NEAREST,\n",
    "    resize_library = ResizeLib.PIL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56d65b",
   "metadata": {},
   "source": [
    "Further, we want our model to segment clouds as well.\n",
    "Hence, we have to apply some mapping again using the [ZipFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#ZipFeatureTask) as we combine two features to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_key_value_zipper(*arrays,key=0,value=0):\n",
    "    reference = arrays[0]\n",
    "    mask = arrays[1].squeeze(0) # squeeze as originally temporal feature type\n",
    "    reference[mask==key] = value\n",
    "    return reference\n",
    "task_reference_cloudmapping = ZipFeatureTask(\n",
    "    input_features = [\n",
    "        (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "        (FeatureType.MASK, \"cmask_data\")\n",
    "    ],\n",
    "    output_feature = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    zip_function = mask_key_value_zipper,\n",
    "    key = 1,\n",
    "    value = config[\"class_clouds\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7fb3e2",
   "metadata": {},
   "source": [
    "## Masking\n",
    "These EOTasks define the data we want to have as an input and as a reference for our problem.\n",
    "Still, **we have areas not providing reasonable data** at all or not in a meaningful way as for indefinite land cover, for example.\n",
    "That is, we take care of our dataMasks and the indefinite data for the reference.\n",
    "\n",
    "For the sake of simplicity we want to **filter out every sample of the input not providing the full data**.\n",
    "This filtering will be done based on the analysis of the [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask) applied to the dataMask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Filter out incomplete input data patches\n",
    "def checker_nodata(array):\n",
    "    return bool(np.all(array))\n",
    "task_data_check = MapFeatureTask(\n",
    "    input_features = (FeatureType.MASK, \"dmask_data\"),\n",
    "    output_features = (FeatureType.META_INFO, \"valid\"),\n",
    "    map_function = checker_nodata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9883f32",
   "metadata": {},
   "source": [
    "For the indefinite land cover in our reference data, we calculate a mask using the [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% apply labelmapping\n",
    "def calculatemask(reference,key):\n",
    "    mask = np.ones(reference.shape, dtype=np.uint8)\n",
    "    mask[reference==key] = 0\n",
    "    return mask\n",
    "task_reference_mask = MapFeatureTask(\n",
    "    input_features = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    output_features = (FeatureType.MASK_TIMELESS, \"mask_reference\"),\n",
    "    map_function = calculatemask,\n",
    "    key = config[\"class_indefinite\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac251d6",
   "metadata": {},
   "source": [
    "## Merging and Saving\n",
    "**Only valid EOPatches are saved** using the [Save-Valid-Task]() based on the citerion regarding the input data availability.\n",
    "Note the **compression** keyword - if not set, the memory consumption may get really large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save EOPatches\n",
    "task_save = SaveValidTask(\n",
    "    feature_to_check = (FeatureType.META_INFO, \"valid\"),\n",
    "    path = config[\"dir_data\"],\n",
    "    filesystem = None,\n",
    "    config = config[\"SHconfig\"],\n",
    "    overwrite_permission = OverwritePermission.OVERWRITE_PATCH,\n",
    "    compress_level = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa903de6",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "Now, we can define a workflow bringing everything together\n",
    "- ### Input\n",
    ">- task_data\n",
    ">- task_data_pick\n",
    ">- task_data_pick_dmask\n",
    ">- task_data_pick_cmask\n",
    ">- task_data_tdigest\n",
    ">- task_data_check\n",
    "\n",
    "- ### Reference\n",
    ">- task_reference\n",
    ">- task_reference_labelmapping\n",
    ">- task_reference_resize\n",
    ">- task_reference_cloudmapping\n",
    ">- task_reference_mask\n",
    "\n",
    "- ### Merging and Saving\n",
    ">- task_save\n",
    "\n",
    "## Define Nodes\n",
    "Let's initialise the nodes we will use for our workflow afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d48e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% input nodes\n",
    "node_data = EONode(\n",
    "    task = task_data,\n",
    "    inputs = [],\n",
    "    name = \"load Sentinel-2 data\"\n",
    ")\n",
    "node_data_pick = EONode(\n",
    "    task = task_data_pick,\n",
    "    inputs = [node_data],\n",
    "    name = \"pick closest observation to reference for data\"\n",
    ")\n",
    "node_data_pick_dmask = EONode(\n",
    "    task = task_data_pick_dmask,\n",
    "    inputs = [node_data_pick],\n",
    "    name = \"pick closest observation to reference for data mask\"\n",
    ")\n",
    "node_data_pick_cmask = EONode(\n",
    "    task = task_data_pick_cmask,\n",
    "    inputs = [node_data_pick_dmask],\n",
    "    name = \"pick closest observation to reference for cloud mask\"\n",
    ")\n",
    "node_data_tdigest = EONode(\n",
    "    task = task_data_tdigest,\n",
    "    inputs = [node_data_pick_cmask],\n",
    "    name = \"compute T-Digest of data\"\n",
    ")\n",
    "node_data_check = EONode(\n",
    "    task = task_data_check,\n",
    "    inputs = [node_data_tdigest],\n",
    "    name = \"check data for completeness\"\n",
    ")\n",
    "\n",
    "#%% reference nodes\n",
    "node_reference = EONode(\n",
    "    task = task_reference,\n",
    "    inputs = [node_data_check],\n",
    "    name = \"load reference from disk\"\n",
    ")\n",
    "node_reference_labelmapping = EONode(\n",
    "    task = task_reference_labelmapping,\n",
    "    inputs = [node_reference],\n",
    "    name = \"apply labelmapping\"\n",
    ")\n",
    "node_reference_resize = EONode(\n",
    "    task = task_reference_resize,\n",
    "    inputs = [node_reference_labelmapping],\n",
    "    name = \"apply labelmapping\"\n",
    ")\n",
    "node_reference_cloudmapping = EONode(\n",
    "    task = task_reference_cloudmapping,\n",
    "    inputs = [node_reference_resize],\n",
    "    name = \"map clouds into reference\"\n",
    ")\n",
    "node_reference_mask = EONode(\n",
    "    task = task_reference_mask,\n",
    "    inputs = [node_reference_cloudmapping],\n",
    "    name = \"mask indefinite reference data\"\n",
    ")\n",
    "\n",
    "#%% merging and saving nodes\n",
    "node_save = EONode(\n",
    "    task = task_save,\n",
    "    inputs = [node_reference_mask],\n",
    "    name = \"save valid EOPatch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e24f87",
   "metadata": {},
   "source": [
    "## Define Workflow\n",
    "Now, we finally can define a workflow based on our tasks and nodes.\n",
    "We could either put every single node in the constructor using a list or define our whole workflow by just the last node: `node_save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42597eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = EOWorkflow.from_endnodes(node_save)\n",
    "#workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1b41c",
   "metadata": {},
   "source": [
    "## Test Workflow\n",
    "Now, we want to test our workflow with some arbitrary patch (from our training set) at some arbitrary date (not included into study)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c477f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "workflow.execute({\n",
    "    node_data: {\"bbox\":bbox_list_train[596],\"time_interval\":(\"2022-10-01\",\"2022-10-01\")},\n",
    "    node_save: {\"eopatch_folder\":\"testpatch\"}\n",
    "})\n",
    "eopatch = EOPatch.load(os.path.join(config[\"dir_data\"],\"testpatch\"))\n",
    "eopatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c25cd",
   "metadata": {},
   "source": [
    "Let's have a look at our eopatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB = eopatch[\"data\"][\"data\"][0,...,np.array([2,1,0])].transpose(1,2,0)\n",
    "cmap = ListedColormap([\"white\",\"blue\",\"darkgreen\",\"orange\",\"black\"])\n",
    "\n",
    "#%% plot testpatch\n",
    "plt.figure()\n",
    "plt.subplot(221)\n",
    "plt.imshow(RGB*2.5)\n",
    "plt.title(\"RGB\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(222)\n",
    "plt.imshow(eopatch[\"mask_timeless\"][\"reference\"],vmin=0,vmax=config[\"num_classes\"],cmap=cmap)\n",
    "plt.title(\"Reference\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(223)\n",
    "plt.imshow(eopatch[\"mask\"][\"cmask_data\"][0,...],vmin=0,vmax=1,cmap=\"YlOrBr_r\")\n",
    "plt.title(\"Clouds\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(224)\n",
    "plt.imshow(eopatch[\"mask_timeless\"][\"mask_reference\"],vmin=0,vmax=1,cmap=\"RdYlGn\")\n",
    "plt.title(\"Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config[\"dir_imgs\"],\"testpatch.png\"),dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d66c2",
   "metadata": {},
   "source": [
    "# Workflow Arguments\n",
    "Now it's time to download the data.\n",
    "Therefore, we have to define workflow arguments, both temporal and spatially.\n",
    "Note that we only want to download the data which does not exist on our device.\n",
    "Hence, we check for existence first and assign arguments afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_args = []\n",
    "for _ in [\"train\",\"validation\",\"test\"]:\n",
    "    print(_)\n",
    "    bbox_list_ = bbox_lists[_]\n",
    "    for i in np.random.randint(0,len(bbox_list_),40 if _==\"train\" else 10):##########################range(len(bbox_list_)):\n",
    "        print(f\"\\r{i+1}/{len(bbox_list_)}\",end=\"\\r\")\n",
    "        try:\n",
    "            timestamps = parse_time_interval_observations(\n",
    "                time_interval = (config[f\"start_{_}\"],config[f\"end_{_}\"]),\n",
    "                bbox = bbox_list_[i], \n",
    "                data_collection = DataCollection.SENTINEL2_L1C, \n",
    "                check_timedelta = config[\"checktimedelta\"],\n",
    "                include_borders = True,\n",
    "                time_difference = dt.timedelta(hours=1,seconds=0), \n",
    "                maxcc = config[\"maxcc\"], \n",
    "                config = config[\"SHconfig\"]\n",
    "            )\n",
    "\n",
    "            dir_ = f\"{_}/eopatch_{i}_{timestamps[0].strftime(r'%Y-%m-%dT%H-%M-%S_%Z')}_{timestamps[1].strftime(r'%Y-%m-%dT%H-%M-%S_%Z')}\"\n",
    "            if not os.path.exists(os.path.join(config[\"dir_data\"],dir_)):### and False: ### \n",
    "                workflow_args.append(\n",
    "                    {\n",
    "                        node_data: {\"bbox\":bbox_list_[i],\"time_interval\":timestamps},\n",
    "                        node_save: {\"eopatch_folder\":dir_}\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print()\n",
    "\n",
    "print(f\"Number of downloads: {len(workflow_args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c73267",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_args[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d96eee",
   "metadata": {},
   "source": [
    "# Executor\n",
    "Our area of interest has been defined, our desired data has been defined, our workflow has been defined, our execution arguments have been defined, our executor...\n",
    "This has to be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define executor\n",
    "executor = EOExecutor(workflow, workflow_args, save_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2000bdd",
   "metadata": {},
   "source": [
    "Let it run!\n",
    "That may take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe2983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% run\n",
    "print(f\"Will start data acquisition using {config['threads']} threads!\")\n",
    "executor.run(workers=config[\"threads\"])\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37928f",
   "metadata": {},
   "source": [
    "# Downloaded Data\n",
    "After a long time, our executor finished with it's work.\n",
    "Let's **check** if there happened anything unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    print(\n",
    "        f\"Execution failed EOPatches with IDs:\\n{failed_ids}\\n\"\n",
    "        f\"For more info check report at {executor.get_report_path()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f591d5",
   "metadata": {},
   "source": [
    "Let's have a look how many `EOPatches` got stored to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of stored train EOPatches: {len(os.listdir(config['dir_train']))}\")\n",
    "print(f\"Number of stored validation EOPatches: {len(os.listdir(config['dir_validation']))}\")\n",
    "print(f\"Number of stored test EOPatches: {len(os.listdir(config['dir_test']))}\")\n",
    "print()\n",
    "print(f\"Number of downloads: {len(workflow_args)}\")\n",
    "print(f\"Total number of EOPatches: {len(os.listdir(config['dir_train']))+len(os.listdir(config['dir_validation']))+len(os.listdir(config['dir_test']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a354aa2",
   "metadata": {},
   "source": [
    "We finally made it!\n",
    "Everything is ready for being used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547988e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FeatureType.DATA.ndim())\n",
    "print(FeatureType.DATA_TIMELESS.ndim())\n",
    "print(FeatureType.LABEL.ndim())\n",
    "print(FeatureType.LABEL_TIMELESS.ndim())\n",
    "print(FeatureType.MASK.ndim())\n",
    "print(FeatureType.MASK_TIMELESS.ndim())\n",
    "print(FeatureType.SCALAR.ndim())\n",
    "print(FeatureType.SCALAR_TIMELESS.ndim())\n",
    "print(FeatureType.VECTOR.ndim())\n",
    "print(FeatureType.VECTOR_TIMELESS.ndim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13b3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
