{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc03e20",
   "metadata": {},
   "source": [
    "# GEM ML Framework Demonstrator - Water Segmentation\n",
    "In these notebooks, we will get a feeling of how the GEM ML framework can be used for the segmentation of water bodies using Sentinel-1 imagery as input and Sentinel-2 based normalized difference water index (NDWI) as a reference.\n",
    "The idea is to use a neural network (NN) model for the analysis.\n",
    "Thanks to the flexibility of the GEM ML framework, the model used can be replaced by changing the configuration only.\n",
    "We will have a look at the following notebooks separately:\n",
    "- 00_Configuration\n",
    "- 01_DataAcquisition\n",
    "- 02_DataNormalization\n",
    "- 03_TrainingValidationTesting\n",
    "- 04_PyTorchTasks_ModelForwardTask\n",
    "\n",
    "by Michael Engel (m.engel@tum.de)\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "# Data Acquisition\n",
    "Here, we define our `EOWorkflow` for the download of our desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394ea28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from folium import plugins as foliumplugins\n",
    "from sentinelhub import DataCollection, UtmZoneSplitter\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from libs.ConfigME import Config\n",
    "from tasks.TDigestTask import TDigestTask\n",
    "from tasks.PickIdxTask import PickIdxTask\n",
    "from tasks.SaveValidTask import SaveValidTask\n",
    "\n",
    "from eolearn.core import EOExecutor, EONode, EOWorkflow, FeatureType, MapFeatureTask, MergeEOPatchesTask, OverwritePermission, ZipFeatureTask\n",
    "from eolearn.io import SentinelHubInputTask, SentinelHubEvalscriptTask, get_available_timestamps\n",
    "from eolearn.mask import JoinMasksTask\n",
    "\n",
    "\n",
    "print(\"Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa751f3e",
   "metadata": {},
   "source": [
    "# Config\n",
    "First, we load our configuration file which provides all information we need throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load configuration file\n",
    "config = Config.LOAD(\"config.dill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52610a57",
   "metadata": {},
   "source": [
    "# Area of Interest\n",
    "Let's load the geojson of our area of interests for training, validation and testing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load geojson files\n",
    "aoi_train = gpd.read_file(config['AOI_train'])\n",
    "aoi_validation = gpd.read_file(config['AOI_validation'])\n",
    "aoi_test = gpd.read_file(config['AOI_test'])\n",
    "\n",
    "#%% find best suitable crs and transform to it\n",
    "crs_train = aoi_train.estimate_utm_crs()\n",
    "aoi_train = aoi_train.to_crs(crs_train)\n",
    "aoi_train = aoi_train.buffer(config['AOIbuffer'])\n",
    "\n",
    "crs_validation = aoi_validation.estimate_utm_crs()\n",
    "aoi_validation = aoi_validation.to_crs(crs_validation)\n",
    "aoi_validation = aoi_validation.buffer(config['AOIbuffer'])\n",
    "\n",
    "crs_test = aoi_test.estimate_utm_crs()\n",
    "aoi_test = aoi_test.to_crs(crs_test)\n",
    "aoi_test = aoi_test.buffer(config['AOIbuffer'])\n",
    "\n",
    "#%% dict for query\n",
    "aois = {\"train\":aoi_train,\n",
    "        \"validation\":aoi_validation,\n",
    "        \"test\":aoi_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681b347",
   "metadata": {},
   "source": [
    "Since our **area of interests are too large**, we **split** them into a set of smaller bboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% calculate and print size\n",
    "aoi_train_shape = aoi_train.geometry.values[0]\n",
    "aoi_train_width = aoi_train_shape.bounds[2]-aoi_train_shape.bounds[0]\n",
    "aoi_train_height = aoi_train_shape.bounds[3]-aoi_train_shape.bounds[1]\n",
    "print(f\"Dimension of the training area is {aoi_train_width:.0f} x {aoi_train_height:.0f} m2\")\n",
    "aoi_validation_shape = aoi_validation.geometry.values[0]\n",
    "aoi_validation_width = aoi_validation_shape.bounds[2]-aoi_validation_shape.bounds[0]\n",
    "aoi_validation_height = aoi_validation_shape.bounds[3]-aoi_validation_shape.bounds[1]\n",
    "print(f\"Dimension of the validation area is {aoi_validation_width:.0f} x {aoi_validation_height:.0f} m2\")\n",
    "aoi_test_shape = aoi_test.geometry.values[0]\n",
    "aoi_test_width = aoi_test_shape.bounds[2]-aoi_test_shape.bounds[0]\n",
    "aoi_test_height = aoi_test_shape.bounds[3]-aoi_test_shape.bounds[1]\n",
    "print(f\"Dimension of the test area is {aoi_test_width:.0f} x {aoi_test_height:.0f} m2\")\n",
    "\n",
    "#%% create a splitter to obtain a list of bboxes\n",
    "bbox_splitter_train = UtmZoneSplitter([aoi_train_shape], aoi_train.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "bbox_splitter_validation = UtmZoneSplitter([aoi_validation_shape], aoi_validation.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "bbox_splitter_test = UtmZoneSplitter([aoi_test_shape], aoi_test.crs, config[\"patchpixelwidth\"]*config[\"resolution\"])\n",
    "\n",
    "bbox_list_train = np.array(bbox_splitter_train.get_bbox_list())\n",
    "info_list_train = np.array(bbox_splitter_train.get_info_list())\n",
    "bbox_list_validation = np.array(bbox_splitter_validation.get_bbox_list())\n",
    "info_list_validation = np.array(bbox_splitter_validation.get_info_list())\n",
    "bbox_list_test = np.array(bbox_splitter_test.get_bbox_list())\n",
    "info_list_test = np.array(bbox_splitter_test.get_info_list())\n",
    "\n",
    "#%% dict for query\n",
    "bbox_lists = {\"train\":bbox_list_train,\n",
    "              \"validation\":bbox_list_validation,\n",
    "              \"test\":bbox_list_test}\n",
    "info_lists = {\"train\":info_list_train,\n",
    "              \"validation\":info_list_validation,\n",
    "              \"test\":info_list_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408ac31",
   "metadata": {},
   "source": [
    "The **bbox list would be sufficient** for starting the training procedure using eo-learn.\n",
    "To check if we muddled up something, however, we want to visualize it!\n",
    "Since our area of interest is rather large, we face the problem of multiple coordinate refernce systems.\n",
    "Unfortunately, **geopandas does not support multiple crs in one dataframe** as described [here](https://github.com/sentinel-hub/sentinelhub-py/issues/123).\n",
    "Hence, we have to define a set of tiles for each separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = []\n",
    "crss_uniques = []\n",
    "for _ in [\"train\",\"validation\",\"test\"]:\n",
    "    tiles.append([])\n",
    "    #%% determine number of coordinate reference systems\n",
    "    crss = [bbox_._crs for bbox_ in bbox_lists[_]]\n",
    "    crss_unique = np.array(list(dict.fromkeys(crss)))\n",
    "    crss_uniques.append(crss_unique)\n",
    "    n_crss = len(crss_unique)\n",
    "\n",
    "    #%% sort geometries and indices by crs and store to disk\n",
    "    geometries = [[] for i in range(n_crss)]\n",
    "    idxs = [[] for i in range(n_crss)]\n",
    "    idxs_x = [[] for i in range(n_crss)]\n",
    "    idxs_y = [[] for i in range(n_crss)]\n",
    "    for i,info in enumerate(info_lists[_]):\n",
    "        idx_ = np.argmax(crss_unique==bbox_lists[_][i]._crs)\n",
    "\n",
    "        geometries[idx_].append(Polygon(bbox_lists[_][i].get_polygon())) # geometries sorted by crs\n",
    "        idxs[idx_].append(info[\"index\"]) # idxs sorted by crs\n",
    "        idxs_x[idx_].append(info[\"index_x\"]) # idxs_x sorted by crs\n",
    "        idxs_y[idx_].append(info[\"index_y\"]) # idxs_y sorted by crs\n",
    "\n",
    "    for i in range(n_crss):\n",
    "        #%% build dataframe of our areas of interest (and each crs)\n",
    "        tiles[-1].append(\n",
    "            gpd.GeoDataFrame(\n",
    "                {\"index\": idxs[i], \"index_x\": idxs_x[i], \"index_y\": idxs_y[i]},\n",
    "                crs=\"EPSG:\"+crss_unique[i]._value_,\n",
    "                geometry=geometries[i]\n",
    "            )\n",
    "        )\n",
    "        #%%% save dataframes to shapefiles\n",
    "        tiles[-1][-1].to_file(os.path.join(config[\"dir_results\"],f\"grid_aoi_{_}_{i}_EPSG{str(crss_unique[i]._value_)}.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfb20c",
   "metadata": {},
   "source": [
    "We have sorted the tiles according to their corresponding crs.\n",
    "Now we want to visualize it in a nice map.\n",
    "Here, it is important to **reproject the tiles** to the crs of our **mapping application** - we do that only for this purpose, the **bbox list is not affected** by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print amount of patches\n",
    "print(\"Total number of tiles:\",[len(bbox_list) for bbox_list in bbox_lists.values()])\n",
    "\n",
    "#%% visualize using folium\n",
    "aoi_folium = aoi_validation.to_crs(\"EPSG:4326\") # use validation for visualisation\n",
    "location = [aoi_folium.centroid.y,aoi_folium.centroid.x]\n",
    "\n",
    "mapwindow = folium.Map(location=location, tiles='Stamen Terrain', zoom_start=6)\n",
    "\n",
    "colors = [\"blue\",\"green\",\"red\"]\n",
    "for i,_ in enumerate([\"train\",\"validation\",\"test\"]):\n",
    "    #%%% add aois\n",
    "    #%%%% train\n",
    "    mapwindow.add_child(\n",
    "        folium.features.Choropleth(\n",
    "            aois[_].to_crs(\"EPSG:4326\").to_json(),\n",
    "            fill_color=colors[i],\n",
    "            nan_fill_color=colors[i],\n",
    "            fill_opacity=0,\n",
    "            nan_fill_opacity=0.5,\n",
    "            line_color=colors[i],\n",
    "            line_weight=1,\n",
    "            line_opacity=0.6,\n",
    "            smooth_factor=5,\n",
    "            name=f\"{_} area\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #%%% add grids in blue color\n",
    "    for t_,tiles_ in enumerate(tiles[i]):\n",
    "        cp = folium.features.Choropleth(\n",
    "                tiles_.to_crs(\"EPSG:4326\").to_json(),\n",
    "                fill_color=colors[i],\n",
    "                nan_fill_color=\"black\",\n",
    "                fill_opacity=0,\n",
    "                nan_fill_opacity=0.5,\n",
    "                line_color=colors[i],\n",
    "                line_weight=0.5,\n",
    "                line_opacity=0.6,\n",
    "                smooth_factor=5,\n",
    "                name=f\"{_} grid EPSG:{crss_uniques[i][t_]._value_}\"\n",
    "            ).add_to(mapwindow)\n",
    "\n",
    "        # display index next to cursor\n",
    "        folium.GeoJsonTooltip(\n",
    "            ['index'],\n",
    "            aliases=['Index:'],\n",
    "            labels=False,\n",
    "            style=\"background-color:rgba(0,101,189,0.4); border:2px solid white; color:white;\",\n",
    "            ).add_to(cp.geojson)\n",
    "\n",
    "#%%% add some controls\n",
    "folium.LayerControl().add_to(mapwindow)\n",
    "foliumplugins.Fullscreen(force_separate_button=True).add_to(mapwindow)\n",
    "\n",
    "#%%% save, render and display\n",
    "mapwindow.save(os.path.join(config[\"dir_results\"],'gridmap.html'))\n",
    "mapwindow.render()\n",
    "mapwindow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2158ac",
   "metadata": {},
   "source": [
    "# Input Tasks\n",
    "Now, it is time to define some input tasks for our `eo-learn` workflows.\n",
    "As an input, we will take a [Sentinel-Hub-Input-Task](https://eo-learn.readthedocs.io/en/latest/eolearn.io.sentinelhub_process.html#eolearn.io.sentinelhub_process.SentinelHubInputTask) for querying **Sentinel-1 data**.\n",
    "As a reference, we will use a [Sentinel-Hub-Evalscript-Task](https://eo-learn.readthedocs.io/en/latest/eolearn.io.sentinelhub_process.html#eolearn.io.sentinelhub_process.SentinelHubEvalscriptTask) with Sentinel-2 data to **calculate the NDWI**.\n",
    "For **cloud masking** we use the mask calculated by S2Cloudless.\n",
    "\n",
    "The date of our reference observations by Sentinel-2 define the time intervals for our input.\n",
    "If one acquisition of Sentinel-2 has been on the 2022-10-07, for example, our input is the Sentinel-1 observation closest to that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sentinel-Hub-Input-Task\n",
    "task_data = SentinelHubInputTask(\n",
    "    data_collection = DataCollection.SENTINEL1_IW,\n",
    "    size = None,\n",
    "    resolution = config[\"resolution\"],\n",
    "    bands_feature = (FeatureType.DATA, \"data\"),\n",
    "    bands = None,\n",
    "    additional_data = (FeatureType.MASK, \"dataMask\", \"dmask_data\"),\n",
    "    evalscript = None,\n",
    "    maxcc = None,\n",
    "    time_difference = dt.timedelta(hours=1),\n",
    "    cache_folder = config[\"dir_cache\"],\n",
    "    max_threads = config[\"threads\"],\n",
    "    config = config[\"SHconfig\"],\n",
    "    bands_dtype = np.float32,\n",
    "    single_scene = False,\n",
    "    mosaicking_order = \"mostRecent\",\n",
    "    aux_request_args = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f2d92",
   "metadata": {},
   "source": [
    "In order to get the closest observation with respect to our observation date, we pick the last one only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Pick-Idx-Task\n",
    "task_pick = PickIdxTask(\n",
    "    in_feature = (FeatureType.DATA, \"data\"),\n",
    "    out_feature = None, # None for replacing in_feature\n",
    "    idx = [[-1],...] # -1 in brackets for keeping dimensions of numpy array\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a628f1f",
   "metadata": {},
   "source": [
    "For the normalization of our dataset we will use the T-Digest algorithm.\n",
    "It is designed for quantile approximation close to the tails which we need for the common quantile scaler in the realm of ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bf582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% T-Digest-Task\n",
    "task_tdigest = TDigestTask(\n",
    "    in_feature = (FeatureType.DATA, 'data'),\n",
    "    out_feature = (FeatureType.SCALAR_TIMELESS, 'tdigest_data'),\n",
    "    mode = None,\n",
    "    pixelwise = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96db3b",
   "metadata": {},
   "source": [
    "# Reference Task\n",
    "The reference is calculated using some thresholded value of the NDWI.\n",
    "To enable the user to use other thresholds after downloading the patches, the raw NDWI and the corresponding bands will be stored within the `EOPatch` as well.\n",
    "Additionally, we will download the RGB bands for visualisation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf65204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sentinel-Hub-Evalscript-Task\n",
    "task_reference = SentinelHubEvalscriptTask(\n",
    "    features = [(FeatureType.DATA_TIMELESS,\"B02\"),\n",
    "                (FeatureType.DATA_TIMELESS,\"B03\"),\n",
    "                (FeatureType.DATA_TIMELESS,\"B04\"),\n",
    "                (FeatureType.DATA_TIMELESS,\"B08\"),\n",
    "                (FeatureType.DATA_TIMELESS,\"B8A\"),\n",
    "                (FeatureType.DATA_TIMELESS,\"B11\"),\n",
    "                (FeatureType.DATA_TIMELESS,\"NDWI\"),\n",
    "                (FeatureType.MASK_TIMELESS,\"reference\"),\n",
    "                (FeatureType.MASK_TIMELESS,\"dmask_reference\"),\n",
    "                (FeatureType.MASK_TIMELESS,\"cmask_reference\")\n",
    "               ],\n",
    "    evalscript = \"\"\"\n",
    "        //VERSION=3\n",
    "        \n",
    "        // evaluation function\n",
    "        function evaluatePixel(samples) {\n",
    "            // calculate NDWI\n",
    "            let val = index(samples.B03,samples.B8A);\n",
    "            \n",
    "            // classify water\n",
    "            let water = 0;\n",
    "            if (val>0.1){\n",
    "                water = 1;\n",
    "            } else {\n",
    "                water = 0;\n",
    "            }\n",
    "            \n",
    "            // return water, dataMask and cloudMask\n",
    "            return {\n",
    "                B02: [samples.B02],\n",
    "                B03: [samples.B03],\n",
    "                B04: [samples.B04],\n",
    "                B08: [samples.B08],\n",
    "                B8A: [samples.B8A],\n",
    "                B11: [samples.B11],\n",
    "                NDWI: [val],\n",
    "                reference: [water],\n",
    "                dmask_reference: [samples.dataMask],\n",
    "                cmask_reference: [!samples.CLM]\n",
    "            };\n",
    "        }\n",
    "\n",
    "        //setup function\n",
    "        function setup() {\n",
    "            return {\n",
    "                input: [{\n",
    "                    bands:[\n",
    "                        \"B02\",\n",
    "                        \"B03\",\n",
    "                        \"B04\",\n",
    "                        \"B08\",\n",
    "                        \"B8A\",\n",
    "                        \"B11\",\n",
    "                        \"dataMask\",\n",
    "                        \"CLM\"\n",
    "                    ],\n",
    "                    units:\"DN\",\n",
    "                }],\n",
    "                output:[\n",
    "                    // output definition of bands\n",
    "                    {\n",
    "                        id: \"B02\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT16\n",
    "                    },\n",
    "                    {\n",
    "                        id: \"B03\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT16\n",
    "                    },\n",
    "                    {\n",
    "                        id: \"B04\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT16\n",
    "                    },\n",
    "                    {\n",
    "                        id: \"B08\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT16\n",
    "                    },\n",
    "                    {\n",
    "                        id: \"B8A\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT16\n",
    "                    },\n",
    "                    {\n",
    "                        id: \"B11\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT16\n",
    "                    },\n",
    "                    \n",
    "                    // output definition of NDWI\n",
    "                    {\n",
    "                        id: \"NDWI\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.FLOAT32\n",
    "                    },\n",
    "                \n",
    "                    // output definition of water label\n",
    "                    {\n",
    "                        id: \"reference\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT8,\n",
    "                    },\n",
    "                    \n",
    "                    // output definition of dataMask\n",
    "                    {\n",
    "                        id: \"dmask_reference\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT8,\n",
    "                    },\n",
    "                    \n",
    "                    // output definition of cloudMask\n",
    "                    {\n",
    "                        id: \"cmask_reference\",\n",
    "                        bands: 1,\n",
    "                        sampleType: SampleType.UINT8,\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \"\"\",\n",
    "    data_collection = DataCollection.SENTINEL2_L1C,\n",
    "    size = None,\n",
    "    resolution = config[\"resolution\"],\n",
    "    maxcc = config[\"maxcc\"],\n",
    "    time_difference = dt.timedelta(hours=1),\n",
    "    cache_folder = config[\"dir_cache\"],\n",
    "    max_threads = config[\"threads\"],\n",
    "    config = config[\"SHconfig\"],\n",
    "    mosaicking_order = \"mostRecent\",\n",
    "    aux_request_args = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7fb3e2",
   "metadata": {},
   "source": [
    "## Masking\n",
    "These EOTasks define the data we want to have as an input and as a reference for our learning problem.\n",
    "Still, **we have areas not providing data** at all or not in a meaningful way as for clouds, for example.\n",
    "That is, we take care of our dataMasks and the cloudMask for the reference.\n",
    "\n",
    "For the sake of simplicity we want to **filter out every sample of the input not providing the full data**.\n",
    "This filtering will be done based on the analysis of the [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask) applied to the dataMask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Filter out incomplete input data patches\n",
    "def checker_nodata(array):\n",
    "    return bool(np.all(array))\n",
    "task_data_check = MapFeatureTask(\n",
    "    input_features = (FeatureType.MASK, \"dmask_data\"),\n",
    "    output_features = (FeatureType.META_INFO, \"valid_data\"),\n",
    "    map_function = checker_nodata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9883f32",
   "metadata": {},
   "source": [
    "**Regarding the reference**, we want to filter out all data not providing **at least 5% water body coverage**.\n",
    "Again, this will be done based on a [MapFeatureTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/core/core_tasks.html#MapFeatureTask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Filter out no water patches\n",
    "def checker_waterdata(array):\n",
    "    return bool(np.count_nonzero(array)>=0.05*array.size)\n",
    "task_reference_check = MapFeatureTask(\n",
    "    input_features = (FeatureType.MASK_TIMELESS, \"reference\"),\n",
    "    output_features = (FeatureType.META_INFO, \"valid_reference\"),\n",
    "    map_function = checker_waterdata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed81719",
   "metadata": {},
   "source": [
    "Both the **validity criterions** have to be **merged to one**.\n",
    "This will be done using the [ZipFeatureTask](https://eo-learn.readthedocs.io/en/latest/_m'odules/eolearn/core/core_tasks.html#ZipFeatureTask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b6bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipper(*arrays):\n",
    "    return bool(np.all([np.all(array) for array in arrays]))\n",
    "task_zip = ZipFeatureTask(\n",
    "    input_features = [(FeatureType.META_INFO, \"valid_data\"),\n",
    "                      (FeatureType.META_INFO, \"valid_reference\")],\n",
    "    output_feature = (FeatureType.META_INFO, \"valid\"),\n",
    "    zip_function = zipper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f396dc",
   "metadata": {},
   "source": [
    "The **mask of our reference** will be taken into account while learning: during optimization and for the metrics of valid EOPatches.\n",
    "The NDWI calculated by Sentinel-2 images is only reasonable on areas where no clouds occur.\n",
    "That is, we mask these out.\n",
    "Both, the **dataMask and the cloudMask will be joined** by the [JoinMasksTask](https://eo-learn.readthedocs.io/en/latest/_modules/eolearn/mask/masking.html#JoinMasksTask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebccd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_reference_mask = JoinMasksTask(\n",
    "    input_features = [(FeatureType.MASK_TIMELESS,\"dmask_reference\"),(FeatureType.MASK_TIMELESS,\"cmask_reference\")],\n",
    "    output_feature = (FeatureType.MASK_TIMELESS, \"mask_reference\"),\n",
    "    join_operation = 'and'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac251d6",
   "metadata": {},
   "source": [
    "## Merging and Saving\n",
    "**Different timestamps of features are not supported** by eo-learn, for now.\n",
    "Accordingly, we have to define our **input and our output separately** as they live in different intervals and **join them together afterwards** using the [Merge-EOPatches-Task](https://eo-learn.readthedocs.io/en/latest/eolearn.core.eodata.html#eolearn.core.eodata.EOPatch.merge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13749b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% merge input and reference\n",
    "task_merge = MergeEOPatchesTask(\n",
    "    time_dependent_op = \"concatenate\", \n",
    "    timeless_op = \"concatenate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e74513",
   "metadata": {},
   "source": [
    "Afterwards, **only valid EOPatches are saved** using the [Save-Valid-Task]() based on the citerions regarding the input data availability and fraction of water in the reference data.\n",
    "Note the **compression** keyword - if not set, the memory consumption may get really large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save EOPatches\n",
    "task_save = SaveValidTask(\n",
    "    feature_to_check = (FeatureType.META_INFO, \"valid\"),\n",
    "    path = config[\"dir_data\"],\n",
    "    filesystem = None,\n",
    "    config = config[\"SHconfig\"],\n",
    "    overwrite_permission = OverwritePermission.OVERWRITE_PATCH,\n",
    "    compress_level = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa903de6",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "Now, we can define a workflow bringing everything together\n",
    "- ### Input\n",
    ">- task_data\n",
    ">- task_pick\n",
    ">- task_tdigest\n",
    ">- task_data_check\n",
    "\n",
    "- ### Reference\n",
    ">- task_reference\n",
    ">- task_reference_mask\n",
    ">- task_reference_check\n",
    "\n",
    "- ### Merging and Saving\n",
    ">- task_merge\n",
    ">- task_zip\n",
    ">- task_save\n",
    "\n",
    "## Define Nodes\n",
    "Let's initialise the nodes we will use for our workflow afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d48e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% input nodes\n",
    "node_data = EONode(\n",
    "    task = task_data,\n",
    "    inputs = [],\n",
    "    name = \"load Sentinel-1 data\"\n",
    ")\n",
    "node_pick = EONode(\n",
    "    task = task_pick,\n",
    "    inputs = [node_data],\n",
    "    name = \"pick closest observation to reference\"\n",
    ")\n",
    "node_tdigest = EONode(\n",
    "    task = task_tdigest,\n",
    "    inputs = [node_pick],\n",
    "    name = \"compute T-Digest\"\n",
    ")\n",
    "node_data_check = EONode(\n",
    "    task = task_data_check,\n",
    "    inputs = [node_tdigest],\n",
    "    name = \"check data for completeness\"\n",
    ")\n",
    "\n",
    "#%% reference nodes\n",
    "node_reference = EONode(\n",
    "    task = task_reference,\n",
    "    inputs = [],\n",
    "    name = \"load Sentinel-2 based reference\"\n",
    ")\n",
    "node_reference_mask = EONode(\n",
    "    task = task_reference_mask,\n",
    "    inputs = [node_reference],\n",
    "    name = \"merge reference masks to one\"\n",
    ")\n",
    "node_reference_check = EONode(\n",
    "    task = task_reference_check,\n",
    "    inputs = [node_reference_mask],\n",
    "    name = \"check reference for completeness\"\n",
    ")\n",
    "\n",
    "#%% merging and saving nodes\n",
    "node_merge = EONode(\n",
    "    task = task_merge,\n",
    "    inputs = [node_data_check,node_reference_check],\n",
    "    name = \"merge data and reference EOPatches\"\n",
    ")\n",
    "node_zip = EONode(\n",
    "    task = task_zip,\n",
    "    inputs = [node_merge],\n",
    "    name = \"zip checking criterions\"\n",
    ")\n",
    "node_save = EONode(\n",
    "    task = task_save,\n",
    "    inputs = [node_zip],\n",
    "    name = \"save valid EOPatch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e24f87",
   "metadata": {},
   "source": [
    "## Define Workflow\n",
    "Now, we finally can define a workflow based on our tasks and nodes.\n",
    "We could either put every single node in the constructor using a list or define our whole workflow by just the last node: `node_save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42597eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = EOWorkflow.from_endnodes(node_save)\n",
    "#workflow.dependency_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d66c2",
   "metadata": {},
   "source": [
    "# Workflow Arguments\n",
    "Now it's time to download the data.\n",
    "Therefore, we have to define workflow arguments, both temporal and spatially.\n",
    "Note that we only want to download the data which does not exist on our device.\n",
    "Hence, we check for existence first and assign arguments afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_args = []\n",
    "for _ in [\"train\",\"validation\",\"test\"]:\n",
    "    print(_)\n",
    "    bbox_list_ = bbox_lists[_]\n",
    "    for i in np.random.randint(0,len(bbox_list_),40 if _==\"train\" else 10):##########################range(len(bbox_list_)):\n",
    "        print(f\"\\r{i+1}/{len(bbox_list_)}\",end=\"\\r\")\n",
    "        #%%% query available timestamps\n",
    "        timestamps_ = get_available_timestamps(\n",
    "            bbox = bbox_list_[i], \n",
    "            data_collection = DataCollection.SENTINEL2_L1C, \n",
    "            time_interval = (config[f\"start_{_}\"],config[f\"end_{_}\"]), \n",
    "            time_difference = dt.timedelta(hours=12, seconds=0), \n",
    "            maxcc = config[\"maxcc\"], \n",
    "            config = config[\"SHconfig\"]\n",
    "        )\n",
    "\n",
    "        #%%% define corresponding arguments\n",
    "        for timestamp_ in timestamps_:\n",
    "            try:\n",
    "                timestamps2_ = get_available_timestamps(\n",
    "                    bbox = bbox_list_[i], \n",
    "                    data_collection = DataCollection.SENTINEL1_IW, \n",
    "                    time_interval = (timestamp_-config[\"datatimedelta\"],timestamp_), \n",
    "                    time_difference = dt.timedelta(hours=0,seconds=0), \n",
    "                    maxcc = None, \n",
    "                    config = config[\"SHconfig\"]\n",
    "                )\n",
    "\n",
    "                if timestamps2_:\n",
    "                    dir_ = f\"{_}/eopatch_{i}_{timestamp_.strftime(r'%Y-%m-%dT%H-%M-%S_%Z')}\"\n",
    "                    if not os.path.exists(os.path.join(config[\"dir_data\"],dir_)):### and False: ### \n",
    "                        workflow_args.append(\n",
    "                            {\n",
    "                                node_data: {\"bbox\":bbox_list_[i],\"time_interval\":(timestamp_-config[\"datatimedelta\"],timestamp_)},\n",
    "                                node_reference: {\"bbox\":bbox_list_[i],\"time_interval\":timestamp_},\n",
    "                                node_save: {\"eopatch_folder\":dir_}\n",
    "                            }\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(e,(timestamp_-config[\"datatimedelta\"],timestamp_))\n",
    "    print()\n",
    "\n",
    "print(f\"Number of downloads: {len(workflow_args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c73267",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_args[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d96eee",
   "metadata": {},
   "source": [
    "# Executor\n",
    "Our area of interest has been defined, our desired data has been defined, our workflow has been defined, our execution arguments have been defined, our executor...\n",
    "This has to be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define executor\n",
    "executor = EOExecutor(workflow, workflow_args, save_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2000bdd",
   "metadata": {},
   "source": [
    "Let it run!\n",
    "That may take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe2983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% run\n",
    "executor.run(workers=config[\"threads\"])\n",
    "executor.make_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37928f",
   "metadata": {},
   "source": [
    "# Downloaded Data\n",
    "After a long time, our executor finished with its work.\n",
    "Let's **check** if there happened anything unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    print(\n",
    "        f\"Execution failed EOPatches with IDs:\\n{failed_ids}\\n\"\n",
    "        f\"For more info check report at {executor.get_report_path()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f591d5",
   "metadata": {},
   "source": [
    "Let's have a look how many `EOPatches` got stored to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of stored train EOPatches: {len(os.listdir(config['dir_train']))}\")\n",
    "print(f\"Number of stored validation EOPatches: {len(os.listdir(config['dir_validation']))}\")\n",
    "print(f\"Number of stored test EOPatches: {len(os.listdir(config['dir_test']))}\")\n",
    "print()\n",
    "print(f\"Number of downloads: {len(workflow_args)}\")\n",
    "print(f\"Total number of EOPatches: {len(os.listdir(config['dir_train']))+len(os.listdir(config['dir_validation']))+len(os.listdir(config['dir_test']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a354aa2",
   "metadata": {},
   "source": [
    "We finally made it!\n",
    "Everything is ready for being used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547988e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FeatureType.DATA.ndim())\n",
    "print(FeatureType.DATA_TIMELESS.ndim())\n",
    "print(FeatureType.LABEL.ndim())\n",
    "print(FeatureType.LABEL_TIMELESS.ndim())\n",
    "print(FeatureType.MASK.ndim())\n",
    "print(FeatureType.MASK_TIMELESS.ndim())\n",
    "print(FeatureType.SCALAR.ndim())\n",
    "print(FeatureType.SCALAR_TIMELESS.ndim())\n",
    "print(FeatureType.VECTOR.ndim())\n",
    "print(FeatureType.VECTOR_TIMELESS.ndim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13b3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
