{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f84f86f",
   "metadata": {},
   "source": [
    "# GEM ML Framework Demonstrator - Water Segmentation\n",
    "In these notebooks, we will get a feeling of how the GEM ML framework can be used for the segmentation of water bodies using Sentinel-1 imagery as input and Sentinel-2 based normalized difference water index (NDWI) as a reference.\n",
    "The idea is to use a neural network (NN) model for the analysis.\n",
    "Thanks to the flexibility of the GEM ML framework, the model used can be replaced by changing the configuration only.\n",
    "We will have a look at the following notebooks separately:\n",
    "- 00_Configuration\n",
    "- 01_DataAcquisition\n",
    "- 02_DataNormalization\n",
    "- 03_TrainingValidationTesting\n",
    "- 04_PyTorchTasks_ModelForwardTask\n",
    "\n",
    "by Michael Engel (m.engel@tum.de)\n",
    "\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "# Training, Validation and Testing\n",
    "In this notebook, we will train, validate and test the model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c981409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import platform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tensorboard import notebook\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from libs.ConfigME import Config, importME\n",
    "from libs.QuantileScaler_eolearn import QuantileScaler_eolearn_tdigest\n",
    "from libs.Dataset_eolearn import Dataset_eolearn\n",
    "from libs import AugmentME\n",
    "from utils.transforms import Torchify\n",
    "\n",
    "from eolearn.core import EOPatch, FeatureType\n",
    "\n",
    "print(\"Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484bf33",
   "metadata": {},
   "source": [
    "# Config\n",
    "First, we load our configuration file which provides all information we need throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load configuration file\n",
    "config = Config.LOAD(\"config.dill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b51e3",
   "metadata": {},
   "source": [
    "# Quantile Scaling\n",
    "As discussed in the second notebook, we want to apply quantile scaling to our data.\n",
    "We load the scaler, we already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler = QuantileScaler_eolearn_tdigest.LOAD(os.path.join(config[\"dir_results\"],config[\"savename_scaler\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452d76c",
   "metadata": {},
   "source": [
    "# Dataoader\n",
    "First, we need to get the paths for all samples within our training, validation and testing datasets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ecd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training samples\n",
    "paths_train = [os.path.join(config[\"dir_train\"],file).replace(\"\\\\\",\"/\") for file in os.listdir(config[\"dir_train\"])]\n",
    "\n",
    "#%% validation samples\n",
    "paths_validation = [os.path.join(config[\"dir_validation\"],file).replace(\"\\\\\",\"/\") for file in os.listdir(config[\"dir_validation\"])]\n",
    "\n",
    "#%% testing samples\n",
    "paths_test = [os.path.join(config[\"dir_test\"],file).replace(\"\\\\\",\"/\") for file in os.listdir(config[\"dir_test\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541cfec",
   "metadata": {},
   "source": [
    "Now, we are ready to define our datasets using the `Dataset_eolearn`!\n",
    "Remember that PyTorch asks for the shape `[batch_size x channels x timestamps x height x width]`.\n",
    "The `QuantileScaler_eolearn_tdigest` handles this by setting `transform=Torchify(1)`.\n",
    "For the reference and the mask, we use the `Torchify` class as provided from the `Dataset_eolearn` package, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training dataset\n",
    "dataset_train = Dataset_eolearn(\n",
    "    paths = paths_train,\n",
    "    feature_data = (FeatureType.DATA,\"data\"),\n",
    "    feature_reference = (FeatureType.MASK_TIMELESS,\"reference\"),\n",
    "    feature_mask = (FeatureType.MASK_TIMELESS,\"mask_reference\"),\n",
    "\n",
    "    transform_data = Scaler,\n",
    "    transform_reference = Torchify(1),\n",
    "    transform_mask = Torchify(1),\n",
    "    \n",
    "    return_idx = True,\n",
    "    return_path = False,\n",
    "\n",
    "    torchdevice = None,\n",
    "    torchtype_data = torch.FloatTensor,\n",
    "    torchtype_reference = torch.LongTensor,\n",
    "    torchtype_mask = torch.LongTensor,\n",
    ")\n",
    "\n",
    "#%% validation dataset\n",
    "dataset_validation = Dataset_eolearn(\n",
    "    paths = paths_validation,\n",
    "    feature_data = (FeatureType.DATA,\"data\"),\n",
    "    feature_reference = (FeatureType.MASK_TIMELESS,\"reference\"),\n",
    "    feature_mask = (FeatureType.MASK_TIMELESS,\"mask_reference\"),\n",
    "\n",
    "    transform_data = Scaler,\n",
    "    transform_reference = Torchify(1),\n",
    "    transform_mask = Torchify(1),\n",
    "    \n",
    "    return_idx = True,\n",
    "    return_path = False,\n",
    "\n",
    "    torchdevice = None,\n",
    "    torchtype_data = torch.FloatTensor,\n",
    "    torchtype_reference = torch.LongTensor,\n",
    "    torchtype_mask = torch.LongTensor,\n",
    ")\n",
    "\n",
    "#%% testing dataset\n",
    "dataset_test = Dataset_eolearn(\n",
    "    paths = paths_test,\n",
    "    feature_data = (FeatureType.DATA,\"data\"),\n",
    "    feature_reference = (FeatureType.MASK_TIMELESS,\"reference\"),\n",
    "    feature_mask = (FeatureType.MASK_TIMELESS,\"mask_reference\"),\n",
    "\n",
    "    transform_data = Scaler,\n",
    "    transform_reference = Torchify(1),\n",
    "    transform_mask = Torchify(1),\n",
    "    \n",
    "    return_idx = True,\n",
    "    return_path = False,\n",
    "\n",
    "    torchdevice = None,\n",
    "    torchtype_data = torch.FloatTensor,\n",
    "    torchtype_reference = torch.LongTensor,\n",
    "    torchtype_mask = torch.LongTensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027aec3a",
   "metadata": {},
   "source": [
    "Let's test our datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = dataset_train[:config[\"batch_size\"]]\n",
    "print('Training Data Shape:',sample_train[0].shape)\n",
    "print('Training Reference Shape:',sample_train[1].shape)\n",
    "print('Training Mask Shape:',sample_train[2].shape)\n",
    "print()\n",
    "\n",
    "sample_validation = dataset_validation[:config[\"batch_size\"]]\n",
    "print('Validation Data Shape:',sample_validation[0].shape)\n",
    "print('Validation Reference Shape:',sample_validation[1].shape)\n",
    "print('Validation Mask Shape:',sample_validation[2].shape)\n",
    "print()\n",
    "\n",
    "sample_test = dataset_test[:config[\"batch_size\"]]\n",
    "print('Testing Data Shape:',sample_test[0].shape)\n",
    "print('Testing Reference Shape:',sample_test[1].shape)\n",
    "print('Testing Mask Shape:',sample_test[2].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4709a",
   "metadata": {},
   "source": [
    "Let's define our dataloader for each dataset.\n",
    "We will double our `batch_size` for validation and testing as no gradient calculation is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% training dataloader\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset = dataset_train,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    sampler = None,\n",
    "    batch_sampler = None,\n",
    "    num_workers = 0 if platform.system()==\"Windows\" else config[\"threads\"],\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    "    drop_last = True,\n",
    "    timeout = 0,\n",
    "    worker_init_fn = None,\n",
    "    multiprocessing_context = None,\n",
    "    generator = None\n",
    ")\n",
    "\n",
    "#%% validation dataloader\n",
    "dataloader_validation = torch.utils.data.DataLoader(\n",
    "    dataset = dataset_validation,\n",
    "    batch_size = config[\"max_batch_size\"]*2,\n",
    "    shuffle = False,\n",
    "    sampler = None,\n",
    "    batch_sampler = None,\n",
    "    num_workers = 0 if platform.system()==\"Windows\" else config[\"threads\"],\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    "    drop_last = True,\n",
    "    timeout = 0,\n",
    "    worker_init_fn = None,\n",
    "    multiprocessing_context = None,\n",
    "    generator = None\n",
    ")\n",
    "\n",
    "#%% testing dataloader\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset = dataset_test,\n",
    "    batch_size = config[\"max_batch_size\"]*2,\n",
    "    shuffle = False,\n",
    "    sampler = None,\n",
    "    batch_sampler = None,\n",
    "    num_workers = 0 if platform.system()==\"Windows\" else config[\"threads\"],\n",
    "    collate_fn = None,\n",
    "    pin_memory = False,\n",
    "    drop_last = True,\n",
    "    timeout = 0,\n",
    "    worker_init_fn = None,\n",
    "    multiprocessing_context = None,\n",
    "    generator = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d85c81",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now, it's time to initialise our model.\n",
    "We will do that using `importME` since we want to keep flexibility with regard to the model architecture used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import model\n",
    "module_model = importME(config[\"module_model\"])\n",
    "\n",
    "#%% initialise model\n",
    "model = module_model(**config[\"kwargs_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccea8f",
   "metadata": {},
   "source": [
    "Now, we want to augment the model such that it fits in our training pipeline.\n",
    "We will add some IO methods such as saving and loading to it.\n",
    "Further, we will add a method for getting the gradients during training.\n",
    "That will be used for a brainwave monitor.\n",
    "The benefit of that gets clear if you think of changing the architecture used but do not mean to change the IO interface of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% general IO\n",
    "AugmentME.augment_IO(model,savekey='save',loadkey='load',mode='torch')\n",
    "\n",
    "#%% checkpoint saving\n",
    "AugmentME.augment_checkpoint(model,key='save_checkpoint',mode='torch')\n",
    "\n",
    "#%% gradient method\n",
    "AugmentME.augment_gradient(model,key='get_gradient',mode=None)\n",
    "\n",
    "#%% number of parameters\n",
    "AugmentME.augment_Ntheta(model,key=\"get_Ntheta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0773f3d",
   "metadata": {},
   "source": [
    "As a test if the augmenting worked, we want to have a look at the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed17c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% number of parameters\n",
    "print(\"Number of parameters:\",model.get_Ntheta())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81285352",
   "metadata": {},
   "source": [
    "# Training\n",
    "Before we can start training our model, we have to define a loss function.\n",
    "We will keep it as flexible as the model itself and use `importME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = importME(config[\"module_loss\"])(**config[\"kwargs_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5545d9e",
   "metadata": {},
   "source": [
    "No optimization without an optimizer! \n",
    "Due to corresponding device issues, we have to send our model to the device before we define our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% send model to device to avoid device errors\n",
    "model.to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabeb0f4",
   "metadata": {},
   "source": [
    "Now, we can define our optimizer with the parameters already been sent to our chosen device!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = importME(config[\"module_optimizer\"])(model.parameters(),**config[\"kwargs_optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42689f0",
   "metadata": {},
   "source": [
    "To assess the performance of our model, we load some metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = importME(config[\"module_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd30da",
   "metadata": {},
   "source": [
    "Of course, we would like to track the proceeding of our training procedure.\n",
    "Hence, we define a tensorboard [SummaryWriter](https://tensorboardx.readthedocs.io/en/latest/tensorboard.html#tensorboardX.SummaryWriter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4148421",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(config[\"dir_tensorboard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae528c2",
   "metadata": {},
   "source": [
    "The tensorboard SummaryWriter enables us to do some nice stuff.\n",
    "For example, we may add some graph of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, sample_train[0].to(config[\"device\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaccd9",
   "metadata": {},
   "source": [
    "Anyway, we would like to make our experiment reproducible.\n",
    "Thus, we set the seeds such that all random number generation and shuffling is done in a deterministic manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13285623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% reproducibility\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b54ce6",
   "metadata": {},
   "source": [
    "In case of a premature exit of the training procedure, we insert a resume flag here.\n",
    "It enables the user to start with the chosen checkpoint or automatically choose the most recent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% resume flag\n",
    "resume = False\n",
    "\n",
    "#%% resume case\n",
    "if resume:\n",
    "    if resume==True:\n",
    "        resume = os.path.join(config[\"dir_checkpoints\"],natsort.natsorted(os.listdir(config[\"dir_checkpoints\"]))[-1])\n",
    "    else:\n",
    "        resume = resume\n",
    "    \n",
    "    print(f'Loading Checkpoint {resume}!')\n",
    "    checkpoint = torch.load(resume,map_location=config[\"device\"])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    loss = checkpoint['loss']\n",
    "    bestloss = checkpoint['bestloss']\n",
    "    epoch_ = checkpoint['epoch']+1\n",
    "    logstep_ = checkpoint['logstep']\n",
    "else:\n",
    "    epoch_ = 0\n",
    "    logstep_ = 0\n",
    "    bestloss = np.inf\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4eadc9",
   "metadata": {},
   "source": [
    "Let's start the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3cd3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% training loop\n",
    "print('Start training...')\n",
    "logstep = -1+logstep_\n",
    "for epoch in range(config[\"n_epochs\"]-epoch_):\n",
    "    epoch = epoch+epoch_\n",
    "    for step, (x, y, mask, idx) in enumerate(dataloader_train):\n",
    "        print('epoch %i step %i'%(epoch,step))\n",
    "        \n",
    "        #%%% clean cache of GPU\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #%%% compute logstep\n",
    "        logstep = logstep+1\n",
    "\n",
    "        #%%% zero gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        #%%% determine number of minibatches\n",
    "        if type(x)==list:\n",
    "            batchcount = int(np.ceil(len(x[0])/config[\"max_batch_size\"]))\n",
    "        else:\n",
    "            batchcount = int(np.ceil(len(x)/config[\"max_batch_size\"]))\n",
    "\n",
    "        out = []\n",
    "        loss = 0\n",
    "        #%%% minibatch-loop\n",
    "        for p in range(batchcount):\n",
    "            #%%%% determine indices\n",
    "            lowidx = p*config[\"max_batch_size\"]\n",
    "            if p==batchcount-1:\n",
    "                if type(x)==list:\n",
    "                    highidx = len(x[0])\n",
    "                else:\n",
    "                    highidx = len(x)\n",
    "            else:\n",
    "                highidx = (p+1)*config[\"max_batch_size\"]\n",
    "                \n",
    "            if type(x)==list:\n",
    "                tmp_x = [torch.index_select(x_,dim=0,index=torch.arange(lowidx,highidx)).detach() for x_ in x]\n",
    "            else:\n",
    "                tmp_x = torch.index_select(x,dim=0,index=torch.arange(lowidx,highidx)).detach()\n",
    "            \n",
    "            tmp_y = torch.index_select(y,dim=0,index=torch.arange(lowidx,highidx)).detach()\n",
    "            tmp_mask = torch.index_select(mask,dim=0,index=torch.arange(lowidx,highidx)).detach()\n",
    "        \n",
    "            #%%%% forward pass\n",
    "            if type(tmp_x)==list:\n",
    "                tmp_out = model.forward([item_.to(config[\"device\"]) for item_ in tmp_x])\n",
    "            else:\n",
    "                tmp_out = model.forward(tmp_x.to(config[\"device\"]))\n",
    "\n",
    "            #%%%% compute loss\n",
    "            tmp_loss = loss_function(tmp_out.softmax(1),tmp_y.squeeze(1).to(config[\"device\"]))\n",
    "            tmp_loss = (tmp_loss*tmp_mask.long().squeeze(1).to(config[\"device\"])).sum() / (torch.count_nonzero(tmp_mask.long().to(config[\"device\"])))\n",
    "\n",
    "            #%%%% compute gradient\n",
    "            tmp_loss.backward()\n",
    "            \n",
    "            #%%%% collect minibatch output\n",
    "            out.append(tmp_out.detach().cpu())\n",
    "            loss = loss+torch.count_nonzero(tmp_mask.long().detach().cpu())/torch.count_nonzero(mask.long().detach().cpu())*tmp_loss.detach().cpu()\n",
    "\n",
    "            #%%%% free space # keep?\n",
    "            del(tmp_x)\n",
    "            del(tmp_y)\n",
    "            del(tmp_mask)\n",
    "            del(tmp_loss)\n",
    "            del(tmp_out)\n",
    "\n",
    "        #%%% update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #%%% compute metric\n",
    "        out = torch.concat(out,dim=0)\n",
    "        if type(metric)==list:\n",
    "            train_acc = [metric_(out,y.cpu().detach(),mask.cpu().detach()) for metric_ in metric]\n",
    "        else:\n",
    "            train_acc = metric(out,y.cpu().detach(),mask.cpu().detach())\n",
    "\n",
    "        #%%% printing stuff\n",
    "        print(\n",
    "            \"[{}] Training Step: {:d}/{:d} {:d}.{:d}, \\tbatch_size: {} \\tLoss: {:.4f} \\tAcc: {}\".format(\n",
    "                dt.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\"),\n",
    "                logstep+1,\n",
    "                len(dataloader_train)*config[\"n_epochs\"],\n",
    "                epoch,\n",
    "                step,\n",
    "                config[\"batch_size\"],\n",
    "                loss.mean(),\n",
    "                {metric_.__name__:train_acc_ for metric_,train_acc_ in zip(metric,train_acc)} if type(metric)==list else train_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #%%% write to tensorboard\n",
    "        #%%%% log loss\n",
    "        writer.add_scalar(f'LossTraining/{type(loss_function).__name__}', loss, global_step=logstep)\n",
    "        \n",
    "        #%%%% log metric\n",
    "        if type(metric)==list:\n",
    "            writer.add_scalars('AccuracyTraining',{metric_.__name__:train_acc_ for metric_,train_acc_ in zip(metric,train_acc)},global_step=logstep)\n",
    "        else:\n",
    "            writer.add_scalar('AccuracyTraining', train_acc, global_step=logstep)\n",
    "        \n",
    "        #%%%% gradients\n",
    "        writer.add_histogram('GradientsTraining/AllParams', model.get_gradient(mode='vec',index=None), global_step=logstep, bins=50, walltime=None, max_bins=100)\n",
    "        for name,grad in model.get_gradient(mode='named params',device=\"cpu\",detach=True):\n",
    "            writer.add_histogram(f'NamedGradientsTraining/{name}', grad, global_step=logstep, bins=50, walltime=None, max_bins=100)\n",
    "        \n",
    "    #%%% intermediate evaluation of validation set\n",
    "    if config[\"eval_freq\"] and (epoch+1)%config[\"eval_freq\"]==0:\n",
    "        print()\n",
    "        model.eval()\n",
    "        loss_val = []\n",
    "        acc_val = []\n",
    "        weights_val = []\n",
    "        with torch.no_grad():\n",
    "            fig, axis = plt.subplots(nrows=len(dataloader_validation)*2, ncols=dataloader_validation.batch_size, figsize=(3*dataloader_validation.batch_size,2*3*len(dataloader_validation)))\n",
    "            fig.suptitle('Validation Data %i'%logstep)                \n",
    "            for step_validation, (x_validation, y_validation, mask_validation, idx_validation) in enumerate(dataloader_validation):\n",
    "                print('validation step %i'%(step_validation))\n",
    "\n",
    "                #%%%% clean cache of GPU\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                #%%%% forward pass\n",
    "                if type(x)==list:\n",
    "                    out_validation = model.forward([item_.to(config[\"device\"]) for item_ in x_validation])\n",
    "                else:\n",
    "                    out_validation = model.forward(x_validation.to(config[\"device\"]))\n",
    "\n",
    "                #%%%% compute loss\n",
    "                loss_validation = loss_function(out_validation.softmax(1),y_validation.squeeze(1).to(config[\"device\"]))\n",
    "                loss_validation = (loss_validation*mask_validation.long().squeeze(1).to(config[\"device\"])).sum() / (torch.count_nonzero(mask_validation.long().to(config[\"device\"])))\n",
    "\n",
    "                #%%%% compute metric\n",
    "                if type(metric)==list:\n",
    "                    validation_acc = [metric_(out_validation.cpu().detach(),y_validation.cpu().detach(),mask_validation.cpu().detach()) for metric_ in metric]\n",
    "                else:\n",
    "                    validation_acc = metric(out_validation.cpu().detach(),y_validation.cpu().detach(),mask_validation.cpu().detach())\n",
    "\n",
    "                #%%%% printing stuff\n",
    "                print(\n",
    "                    \"[{}] Validation Step: {:d}/{:d}, \\tbatch_size: {} \\tLoss: {:.4f} \\tAcc: {}\".format(\n",
    "                        dt.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\"),\n",
    "                        step_validation+1,\n",
    "                        len(dataloader_validation),\n",
    "                        dataloader_validation.batch_size,\n",
    "                        loss_validation.mean(),\n",
    "                        {metric_.__name__:validation_acc_ for metric_,validation_acc_ in zip(metric,validation_acc)} if type(metric)==list else validation_acc\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                #%%%% collect predictions\n",
    "                predictions_validation = torch.argmax(out_validation,1).cpu().detach().numpy()\n",
    "                \n",
    "                axis[step_validation*2][0].set_ylabel(\"Prediction\")\n",
    "                axis[step_validation*2+1][0].set_ylabel(\"Reference\")\n",
    "                for i in range(dataloader_validation.batch_size):\n",
    "                    axis[step_validation*2][i].imshow(predictions_validation[i].squeeze(),vmin=0,vmax=config[\"num_classes\"]-1,cmap=\"YlGnBu\")\n",
    "                    axis[step_validation*2][i].set_xticks([])\n",
    "                    axis[step_validation*2][i].set_yticks([])\n",
    "                    \n",
    "                    axis[step_validation*2+1][i].imshow(y_validation.cpu().detach().numpy()[i].squeeze(),vmin=0,vmax=config[\"num_classes\"]-1,cmap=\"RdBu\")\n",
    "                    axis[step_validation*2+1][i].set_xticks([])\n",
    "                    axis[step_validation*2+1][i].set_yticks([])\n",
    "                    \n",
    "                #%%%% collect loss and accuracy\n",
    "                loss_val.append(loss_validation.cpu().detach().numpy())\n",
    "                acc_val.append(validation_acc)\n",
    "                weights_val.append(torch.count_nonzero(mask_validation).cpu().detach().numpy())\n",
    "\n",
    "            #%%%% total loss and accuracy\n",
    "            total = np.sum([np.sum(weight_) for weight_ in weights_val])\n",
    "            loss_val_total = np.sum([weight_/total*loss_ for weight_,loss_ in zip(weights_val,loss_val)])\n",
    "            if type(metric)==list:\n",
    "                acc_val_total = [np.sum([weight_/total*acc_[i] for weight_,acc_ in zip(weights_val,acc_val)]) for i in range(len(metric))]\n",
    "            else:\n",
    "                acc_val_total = np.sum([weight_/total*acc_ for weight_,acc_ in zip(weights_val,acc_val)])\n",
    "            \n",
    "            # print total values\n",
    "            print(\n",
    "                \"[{}] Validation: \\tTotal Loss: {:.4f} \\tTotal Acc: {}\".format(\n",
    "                    dt.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\"),\n",
    "                    loss_val_total,\n",
    "                    {metric_.__name__:validation_acc_ for metric_,validation_acc_ in zip(metric,acc_val_total)} if type(metric)==list else acc_val_total\n",
    "                )\n",
    "            )\n",
    "\n",
    "            #%%%% write to tensorboard\n",
    "            #%%%%% log loss\n",
    "            writer.add_scalar(f'LossValidation/{type(loss_function).__name__}', loss_val_total, global_step=logstep)\n",
    "\n",
    "            #%%%%% log metric\n",
    "            if type(metric)==list:\n",
    "                writer.add_scalars('AccuracyValidation',{metric_.__name__:validation_acc_ for metric_,validation_acc_ in zip(metric,acc_val_total)},global_step=logstep)\n",
    "            else:\n",
    "                writer.add_scalar('AccuracyValidation', acc_val_total, global_step=logstep)\n",
    "            \n",
    "            #%%%%% log figure\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fname=os.path.join(config[\"dir_imgs_validation\"],\"PredictionValidation_%i\"%logstep), dpi=\"figure\")\n",
    "            writer.add_figure(tag=\"PredictionValidation\", figure=fig, global_step=logstep, close=True, walltime=None)\n",
    "\n",
    "        model.train()\n",
    "        print()\n",
    "        #%%% checkpoint for best validation loss\n",
    "        if config[\"checkpoint_bestloss\"] and bestloss>loss_val_total:\n",
    "            print(\"New best validation loss! Storing checkpoint and model!\")\n",
    "            model.save_checkpoint(\n",
    "                savename = os.path.join(config[\"dir_checkpoints\"],'checkpoint_bestloss.tar'),\n",
    "                epoch = epoch,\n",
    "                logstep = logstep,\n",
    "                optimizer_state_dict = optimizer.state_dict(),\n",
    "                loss = loss,\n",
    "                bestloss = loss_val_total\n",
    "            )\n",
    "            model.save(savename=os.path.join(config[\"dir_results\"],config[\"model_savename_inference_bestloss\"]),mode='inference')\n",
    "            model.save(savename=os.path.join(config[\"dir_results\"],config[\"model_savename_bestloss\"]),mode='entirely')\n",
    "            bestloss = loss_val_total\n",
    "\n",
    "    #%%% checkpoint\n",
    "    if config[\"checkpoint_freq\"] and (epoch+1)%config[\"checkpoint_freq\"]==0:\n",
    "        model.save_checkpoint(\n",
    "            savename = os.path.join(config[\"dir_checkpoints\"],f'checkpoint_{logstep}_{epoch}_{step}.tar'),\n",
    "            epoch = epoch,\n",
    "            logstep = logstep,\n",
    "            optimizer_state_dict = optimizer.state_dict(),\n",
    "            loss = loss,\n",
    "            bestloss = loss_val_total\n",
    "        )\n",
    "\n",
    "#%% save model\n",
    "print('saving final checkpoint!')\n",
    "model.save_checkpoint(savename=os.path.join(config[\"dir_checkpoints\"],f'checkpoint_{logstep}_{epoch}_{step}.tar'), epoch=epoch, logstep=logstep, optimizer_state_dict=optimizer.state_dict(), loss=loss)\n",
    "print('saving inference model')\n",
    "model.save(savename=os.path.join(config[\"dir_results\"],config[\"model_savename_inference\"]),mode='inference')\n",
    "print('saving entire model')\n",
    "model.save(savename=os.path.join(config[\"dir_results\"],config[\"model_savename\"]),mode='entirely')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26aaae",
   "metadata": {},
   "source": [
    "# Testing\n",
    "After the long training, we would like to test our model on the chosen test tiles.\n",
    "\n",
    "However, we load the model providing the best validation loss.\n",
    "For that, we will use the `BaseClass` from AugmentME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AugmentME.BaseClass(mode=\"torch\")\n",
    "model.load(os.path.join(config[\"dir_results\"],config[\"model_savename_bestloss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7644e8e",
   "metadata": {},
   "source": [
    "Since the testing script is rather similar to the validation part of our training procedure, we do not discuss this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19daf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% testing loop\n",
    "print('Start testing...')\n",
    "model.eval()\n",
    "losss_test = []\n",
    "accs_test = []\n",
    "weights_test = []\n",
    "with torch.no_grad():              \n",
    "    for step_test, (x_test, y_test, mask_test, idx_test) in enumerate(dataloader_test):\n",
    "        print('Test step %i'%(step_test))\n",
    "\n",
    "        #%%%% clean cache of GPU\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        #%%%% forward pass\n",
    "        if type(x_test)==list:\n",
    "            out_test = model.forward([item_.to(config[\"device\"]) for item_ in x_test])\n",
    "        else:\n",
    "            out_test = model.forward(x_test.to(config[\"device\"]))\n",
    "\n",
    "        #%%%% compute loss\n",
    "        loss_test = loss_function(out_test.softmax(1),y_test.squeeze(1).to(config[\"device\"]))\n",
    "        loss_test = (loss_test*mask_test.long().squeeze(1).to(config[\"device\"])).sum() / (torch.count_nonzero(mask_test.long().to(config[\"device\"])))\n",
    "\n",
    "        #%%%% compute metric\n",
    "        if type(metric)==list:\n",
    "            test_acc = [metric_(out_test.cpu().detach(),y_test.cpu().detach(),mask_test.cpu().detach()) for metric_ in metric]\n",
    "        else:\n",
    "            test_acc = metric(out_test.cpu().detach(),y_test.cpu().detach(),mask_test.cpu().detach())\n",
    "\n",
    "        #%%%% printing stuff\n",
    "        print(\n",
    "            \"[{}] Test Step: {:d}/{:d}, \\tbatch_size: {} \\tLoss: {:.4f} \\tAcc: {}\".format(\n",
    "                dt.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\"),\n",
    "                step_test+1,\n",
    "                len(dataloader_test),\n",
    "                dataloader_test.batch_size,\n",
    "                loss_test.mean(),\n",
    "                {metric_.__name__:test_acc_ for metric_,test_acc_ in zip(metric,test_acc)} if type(metric)==list else test_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #%%%% collect loss and accuracy\n",
    "        losss_test.append(loss_test.cpu().detach().numpy())\n",
    "        accs_test.append(test_acc)\n",
    "        weights_test.append(torch.count_nonzero(mask_test).cpu().detach().numpy())\n",
    "        \n",
    "        #%%%% plot\n",
    "        #%%%%% calculations for plot\n",
    "        prediction_test = torch.argmax(out_test,1).cpu()\n",
    "        eopatches = [EOPatch.load(dataset_test.paths[idx_.cpu()]) for idx_ in idx_test[0]]\n",
    "        imgs_swir = [np.stack(\n",
    "            (eopatch[(FeatureType.DATA_TIMELESS,\"B11\")].squeeze(-1),\n",
    "             eopatch[(FeatureType.DATA_TIMELESS,\"B08\")].squeeze(-1),\n",
    "             eopatch[(FeatureType.DATA_TIMELESS,\"B04\")].squeeze(-1)),\n",
    "            axis = -1\n",
    "        ) for eopatch in eopatches]\n",
    "        imgs_true = [np.stack(\n",
    "            (eopatch[(FeatureType.DATA_TIMELESS,\"B04\")].squeeze(-1),\n",
    "             eopatch[(FeatureType.DATA_TIMELESS,\"B03\")].squeeze(-1),\n",
    "             eopatch[(FeatureType.DATA_TIMELESS,\"B02\")].squeeze(-1)),\n",
    "            axis = -1\n",
    "        ) for eopatch in eopatches]\n",
    "        \n",
    "        #%%%%% batch plot\n",
    "        fig, axis = plt.subplots(nrows=4, ncols=dataloader_test.batch_size, figsize=(5*dataloader_test.batch_size,5*4))\n",
    "        axis[0][0].set_ylabel(\"Prediction\")\n",
    "        axis[1][0].set_ylabel(\"Reference\")\n",
    "        axis[2][0].set_ylabel(\"SWIR Image\")\n",
    "        axis[3][0].set_ylabel(\"True Color Image with Prediction\")\n",
    "        for i in range(dataloader_test.batch_size):\n",
    "            axis[0][i].imshow(prediction_test[i],vmin=0,vmax=config[\"num_classes\"]-1,cmap=\"YlGnBu\")\n",
    "            axis[0][i].set_yticks([])\n",
    "            axis[0][i].set_xticks([])\n",
    "            \n",
    "            axis[1][i].imshow(y_test.squeeze(1)[i].cpu(),vmin=0,vmax=config[\"num_classes\"]-1,cmap=\"RdBu\")\n",
    "            axis[1][i].set_yticks([])\n",
    "            axis[1][i].set_xticks([])\n",
    "            \n",
    "            axis[2][i].imshow(imgs_swir[i]/np.max(imgs_swir[i]),vmin=0,vmax=config[\"num_classes\"]-1,cmap=\"RdBu\")\n",
    "            axis[2][i].set_yticks([])\n",
    "            axis[2][i].set_xticks([])\n",
    "            \n",
    "            axis[3][i].imshow(imgs_true[i]/np.max(imgs_true[i]),vmin=0,vmax=config[\"num_classes\"]-1,cmap=\"RdBu\")\n",
    "            axis[3][i].set_yticks([])\n",
    "            axis[3][i].set_xticks([])\n",
    "        plt.subplots_adjust(left=0, bottom=0.05, right=1, top=0.95, wspace=0.1, hspace=0)\n",
    "        plt.show()\n",
    "\n",
    "    #%%%% total loss and accuracy\n",
    "    total = np.sum([np.sum(weight_) for weight_ in weights_test])\n",
    "    loss_test_total = np.sum([weight_/total*loss_ for weight_,loss_ in zip(weights_test,losss_test)])\n",
    "    if type(metric)==list:\n",
    "        acc_test_total = [np.sum([weight_/total*acc_[i] for weight_,acc_ in zip(weights_test,accs_test)]) for i in range(len(metric))]\n",
    "    else:\n",
    "        acc_test_total = np.sum([weight_/total*acc_ for weight_,acc_ in zip(weights_test,accs_test)])\n",
    "\n",
    "    # print total values\n",
    "    print(\n",
    "        \"[{}] Test: \\tTotal Loss: {:.4f} \\tTotal Acc: {}\".format(\n",
    "            dt.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\"),\n",
    "            loss_test_total,\n",
    "            {metric_.__name__:test_acc_ for metric_,test_acc_ in zip(metric,acc_test_total)} if type(metric)==list else acc_test_total\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #%%% write to tensorboard\n",
    "    #%%%% log loss\n",
    "    writer.add_scalar(f'LossTest/{type(loss_function).__name__}', loss_test_total, global_step=step_test)\n",
    "\n",
    "    #%%%% log metric\n",
    "    if type(metric)==list:\n",
    "        writer.add_scalars('AccuracyTest',{metric_.__name__:test_acc_ for metric_,test_acc_ in zip(metric,acc_test_total)},global_step=step_test)\n",
    "    else:\n",
    "        writer.add_scalar('AccuracyTest', acc_test_total, global_step=step_test)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071d8d1",
   "metadata": {},
   "source": [
    "Our model has been trained and tested.\n",
    "Hence, we free our GPU from it and it's corresponding variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(optimizer)\n",
    "del(x)\n",
    "del(y)\n",
    "del(mask)\n",
    "del(x_validation)\n",
    "del(y_validation)\n",
    "del(mask_validation)\n",
    "del(x_test)\n",
    "del(y_test)\n",
    "del(mask_test)\n",
    "del(loss)\n",
    "del(loss_validation)\n",
    "del(loss_test)\n",
    "del(grad)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81601344",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Finally and after a long time doing training, validation and testing, we may have a look at the tensorboard.\n",
    "Please make sure, that the tensorboard is running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.list()\n",
    "print('\\nPlease check, if the port is correct and tensorboard is running!\\n')\n",
    "notebook.display(port=6006,height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99d094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
